{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled2.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyO2YTcgcwqAT2KplRmq9sza",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/etesys/numerai/blob/main/Untitled2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qv11kHM2oppF"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from lightgbm import LGBMRegressor\n",
        "import gc\n",
        "from numerapi import NumerAPI\n",
        "from utils import save_prediction, save_model, load_model, neutralize, get_biggest_change_features, validation_metrics, download_data, \\\n",
        "    load_model_config, save_model_config, get_time_series_cross_val_splits\n",
        "\n",
        "\n",
        "EXAMPLE_PREDS_COL = \"example_preds\"\n",
        "TARGET_COL = \"target\"\n",
        "ERA_COL = \"era\"\n",
        "# params we'll use to train all of our models.\n",
        "# Ideal params would be more like 20000, 0.001, 6, 2**6, 0.1, but this is slow enough as it is\n",
        "model_params = {\"n_estimators\": 2000,\n",
        "                \"learning_rate\": 0.01,\n",
        "                \"max_depth\": 5,\n",
        "                \"num_leaves\": 2 ** 5,\n",
        "                \"colsample_bytree\": 0.1}\n",
        "\n",
        "# the amount of downsampling we'll use to speed up cross validation and full train.\n",
        "# a value of 1 means no downsampling\n",
        "# a value of 10 means use every 10th row\n",
        "downsample_cross_val = 20\n",
        "downsample_full_train = 1\n",
        "\n",
        "# if model_selection_loop=True get OOS performance for training_data\n",
        "# and use that to select best model\n",
        "# if model_selection_loop=False, just predict on tournament data using existing models and model config\n",
        "model_selection_loop = True\n",
        "model_config_name = \"advanced_example_model\"\n",
        "\n",
        "napi = NumerAPI()\n",
        "\n",
        "current_round = napi.get_current_round(tournament=8)  # tournament 8 is the primary Numerai Tournament\n",
        "\n",
        "print(\"Entering model selection loop.  This may take awhile.\")\n",
        "if model_selection_loop:\n",
        "    model_config = {}\n",
        "    print('downloading training_data')\n",
        "    download_data(napi, 'numerai_training_data.parquet', 'numerai_training_data.parquet')\n",
        "\n",
        "    print(\"reading training data from local file\")\n",
        "    training_data = pd.read_parquet('numerai_training_data.parquet')\n",
        "\n",
        "    # keep track of some prediction columns\n",
        "    ensemble_cols = set()\n",
        "    pred_cols = set()\n",
        "\n",
        "    # pick some targets to use\n",
        "    possible_targets = [c for c in training_data.columns if c.startswith(\"target_\")]\n",
        "    # randomly pick a handful of targets\n",
        "    # this can be vastly improved\n",
        "    targets = [\"target\", \"target_nomi_60\", \"target_jerome_20\"]\n",
        "\n",
        "    # all the possible features to train on\n",
        "    feature_cols = [c for c in training_data if c.startswith(\"feature_\")]\n",
        "\n",
        "    \"\"\" do cross val to get out of sample training preds\"\"\"\n",
        "    cv = 3\n",
        "    train_test_zip = get_time_series_cross_val_splits(training_data, cv=cv, embargo=12)\n",
        "    # get out of sample training preds via embargoed time series cross validation\n",
        "    # optionally downsample training data to speed up this section.\n",
        "    print(\"entering time series cross validation loop\")\n",
        "    for split, train_test_split in enumerate(train_test_zip):\n",
        "        gc.collect()\n",
        "        print(f\"doing split {split+1} out of {cv}\")\n",
        "        train_split, test_split = train_test_split\n",
        "        train_split_index = training_data[ERA_COL].isin(train_split)\n",
        "        test_split_index = training_data[ERA_COL].isin(test_split)\n",
        "        downsampled_train_split_index = train_split_index[train_split_index].index[::downsample_cross_val]\n",
        "\n",
        "        # getting the per era correlation of each feature vs the primary target across the training split\n",
        "        print(\"getting feature correlations over time and identifying riskiest features\")\n",
        "        all_feature_corrs_split = training_data.loc[downsampled_train_split_index, :].groupby(ERA_COL).apply(\n",
        "            lambda d: d[feature_cols].corrwith(d[TARGET_COL]))\n",
        "        # find the riskiest features by comparing their correlation vs the target in half 1 and half 2 of training data\n",
        "        # there are probably more clever ways to do this\n",
        "        riskiest_features_split = get_biggest_change_features(all_feature_corrs_split, 50)\n",
        "\n",
        "        print(f\"entering model training loop for split {split+1}\")\n",
        "        for target in targets:\n",
        "            model_name = f\"model_{target}\"\n",
        "            print(f\"model: {model_name}\")\n",
        "\n",
        "            # train a model on the training split (and save it for future use)\n",
        "            split_model_name = f\"model_{target}_split{split+1}cv{cv}downsample{downsample_cross_val}\"\n",
        "            split_model = load_model(split_model_name)\n",
        "            if not split_model:\n",
        "                print(f\"training model: {model_name}\")\n",
        "                split_model = LGBMRegressor(**model_params)\n",
        "                split_model.fit(training_data.loc[downsampled_train_split_index, feature_cols],\n",
        "                                training_data.loc[downsampled_train_split_index,\n",
        "                                                  [target]])\n",
        "                save_model(split_model, split_model_name)\n",
        "            # now we can predict on the test part of the split\n",
        "            model_expected_features = split_model.booster_.feature_name()\n",
        "            if set(model_expected_features) != set(feature_cols):\n",
        "                print(f\"New features are available! Might want to retrain model {split_model_name}.\")\n",
        "            print(f\"predicting {model_name}\")\n",
        "            training_data.loc[test_split_index, f\"preds_{model_name}\"] = \\\n",
        "                split_model.predict(training_data.loc[test_split_index, model_expected_features])\n",
        "\n",
        "            # do neutralization\n",
        "            print(\"doing neutralization to riskiest features\")\n",
        "            training_data.loc[test_split_index, f\"preds_{model_name}_neutral_riskiest_50\"] = neutralize(\n",
        "                df=training_data.loc[test_split_index, :],\n",
        "                columns=[f\"preds_{model_name}\"],\n",
        "                neutralizers=riskiest_features_split,\n",
        "                proportion=1.0,\n",
        "                normalize=True,\n",
        "                era_col=ERA_COL)[f\"preds_{model_name}\"]\n",
        "\n",
        "            # remember that we made all of these different pred columns\n",
        "            pred_cols.add(f\"preds_{model_name}\")\n",
        "            pred_cols.add(f\"preds_{model_name}_neutral_riskiest_50\")\n",
        "\n",
        "        print(\"creating ensembles\")\n",
        "        # ranking per era for all of our pred cols so we can combine safely on the same scales\n",
        "        training_data[list(pred_cols)] = training_data.groupby(ERA_COL).apply(\n",
        "            lambda d: d[list(pred_cols)].rank(pct=True))\n",
        "        # do ensembles\n",
        "        training_data[\"ensemble_neutral_riskiest_50\"] = sum(\n",
        "            [training_data[pred_col] for pred_col in pred_cols if pred_col.endswith(\"neutral_riskiest_50\")]).rank(\n",
        "            pct=True)\n",
        "        training_data[\"ensemble_not_neutral\"] = sum(\n",
        "            [training_data[pred_col] for pred_col in pred_cols if \"neutral\" not in pred_col]).rank(pct=True)\n",
        "        training_data[\"ensemble_all\"] = sum([training_data[pred_col] for pred_col in pred_cols]).rank(pct=True)\n",
        "\n",
        "        ensemble_cols.add(\"ensemble_neutral_riskiest_50\")\n",
        "        ensemble_cols.add(\"ensemble_not_neutral\")\n",
        "        ensemble_cols.add(\"ensemble_all\")\n",
        "\n",
        "    \"\"\" Now get some stats and pick our favorite model\"\"\"\n",
        "    print(\"gathering validation metrics for out of sample training results\")\n",
        "    all_model_cols = list(pred_cols) + list(ensemble_cols)\n",
        "    # use example_col preds_model_target as an estimates since no example preds provided for training\n",
        "    # fast_mode=True so that we skip some of the stats that are slower to calculate\n",
        "    training_stats = validation_metrics(training_data, all_model_cols, example_col=\"preds_model_target\",\n",
        "                                        fast_mode=True)\n",
        "    print(training_stats[[\"mean\", \"sharpe\"]].sort_values(by=\"sharpe\", ascending=False).to_markdown())\n",
        "\n",
        "    # pick the model that has the highest correlation sharpe\n",
        "    best_pred_col = training_stats.sort_values(by=\"sharpe\", ascending=False).head(1).index[0]\n",
        "    print(f\"selecting model {best_pred_col} as our highest sharpe model in validation\")\n",
        "\n",
        "    \"\"\" Now do a full train\"\"\"\n",
        "    print(\"entering full training section\")\n",
        "    # getting the per era correlation of each feature vs the target across all of training data\n",
        "    print(\"getting feature correlations with target and identifying riskiest features\")\n",
        "    all_feature_corrs = training_data.groupby(ERA_COL).apply(\n",
        "        lambda d: d[feature_cols].corrwith(d[TARGET_COL]))\n",
        "    # find the riskiest features by comparing their correlation vs the target in half 1 and half 2 of training data\n",
        "    riskiest_features = get_biggest_change_features(all_feature_corrs, 50)\n",
        "\n",
        "    for target in targets:\n",
        "        gc.collect()\n",
        "        model_name = f\"model_{target}_downsample{downsample_full_train}\"\n",
        "        model = load_model(model_name)\n",
        "        if not model:\n",
        "            print(f\"training {model_name}\")\n",
        "            model = LGBMRegressor(**model_params)\n",
        "            # train on all of train, predict on val, predict on tournament\n",
        "            model.fit(training_data.iloc[::downsample_full_train].loc[:, feature_cols],\n",
        "                      training_data.iloc[::downsample_full_train][target])\n",
        "            save_model(model, model_name)\n",
        "        gc.collect()\n",
        "\n",
        "    model_config[\"feature_cols\"] = feature_cols\n",
        "    model_config[\"targets\"] = targets\n",
        "    model_config[\"best_pred_col\"] = best_pred_col\n",
        "    model_config[\"riskiest_features\"] = riskiest_features\n",
        "    print(f\"saving model config for {model_config_name}\")\n",
        "    save_model_config(model_config, model_config_name)\n",
        "else:\n",
        "    # load model config from previous model selection loop\n",
        "    print(f\"loading model config for {model_config_name}\")\n",
        "    model_config = load_model_config(model_config_name)\n",
        "    feature_cols = model_config[\"feature_cols\"]\n",
        "    targets = model_config[\"targets\"]\n",
        "    best_pred_col = model_config[\"best_pred_col\"]\n",
        "    riskiest_features = model_config[\"riskiest_features\"]\n",
        "\n",
        "\n",
        "\"\"\" Things that we always do even if we've already trained \"\"\"\n",
        "gc.collect()\n",
        "print(\"downloading tournament_data\")\n",
        "download_data(napi, 'numerai_tournament_data.parquet', f'numerai_tournament_data_{current_round}.parquet')\n",
        "print(\"downloading validation_data\")\n",
        "download_data(napi, 'numerai_validation_data.parquet', 'numerai_validation_data.parquet')\n",
        "print(\"downloading example_predictions\")\n",
        "download_data(napi, 'example_predictions.parquet', f'example_predictions_{current_round}.parquet')\n",
        "print(\"downloading example_validation_predictions\")\n",
        "download_data(napi, 'example_validation_predictions.parquet', f'example_validation_predictions.parquet')\n",
        "\n",
        "print(\"reading tournament_data\")\n",
        "tournament_data = pd.read_parquet(f'numerai_tournament_data_{current_round}.parquet')\n",
        "print(\"reading validation_data\")\n",
        "validation_data = pd.read_parquet('numerai_validation_data.parquet')\n",
        "print(\"reading example_predictions\")\n",
        "example_preds = pd.read_parquet(f'example_predictions_{current_round}.parquet')\n",
        "print(\"reading example_validaton_predictions\")\n",
        "validation_example_preds = pd.read_parquet('example_validation_predictions.parquet')\n",
        "# set the example predictions\n",
        "validation_data[EXAMPLE_PREDS_COL] = validation_example_preds[\"prediction\"]\n",
        "\n",
        "# check for nans and fill nans\n",
        "print(\"checking for nans in the tournament data\")\n",
        "if tournament_data.loc[tournament_data[\"data_type\"] == \"live\", feature_cols].isna().sum().sum():\n",
        "    cols_w_nan = tournament_data.loc[tournament_data[\"data_type\"] == \"live\", feature_cols].isna().sum()\n",
        "    total_rows = tournament_data[tournament_data[\"data_type\"] == \"live\"]\n",
        "    print(f\"Number of nans per column this week: {cols_w_nan[cols_w_nan > 0]}\")\n",
        "    print(f\"out of {total_rows} total rows\")\n",
        "    print(f\"filling nans with 0.5\")\n",
        "    tournament_data.loc[:, feature_cols] = tournament_data.loc[:, feature_cols].fillna(0.5)\n",
        "else:\n",
        "    print(\"No nans in the features this week!\")\n",
        "\n",
        "\n",
        "pred_cols = set()\n",
        "ensemble_cols = set()\n",
        "for target in targets:\n",
        "    gc.collect()\n",
        "    model_name = f\"model_{target}_downsample{downsample_full_train}\"\n",
        "    print(f\"loading {model_name}\")\n",
        "    model = load_model(model_name)\n",
        "    if not model:\n",
        "        raise ValueError(f\"{model_name} is not trained yet!\")\n",
        "\n",
        "    model_expected_features = model.booster_.feature_name()\n",
        "    if set(model_expected_features) != set(feature_cols):\n",
        "        print(f\"New features are available! Might want to retrain model {model_name}.\")\n",
        "    print(f\"predicting tournament and validation for {model_name}\")\n",
        "    validation_data.loc[:, f\"preds_{model_name}\"] = model.predict(validation_data.loc[:, model_expected_features])\n",
        "    tournament_data.loc[:, f\"preds_{model_name}\"] = model.predict(tournament_data.loc[:, model_expected_features])\n",
        "\n",
        "    # do different neutralizations\n",
        "    # neutralize our predictions to the riskiest features only\n",
        "    print(\"neutralizing to riskiest_50 for validation and tournament\")\n",
        "    validation_data[f\"preds_{model_name}_neutral_riskiest_50\"] = neutralize(df=validation_data,\n",
        "                                                                            columns=[f\"preds_{model_name}\"],\n",
        "                                                                            neutralizers=riskiest_features,\n",
        "                                                                            proportion=1.0,\n",
        "                                                                            normalize=True,\n",
        "                                                                            era_col=ERA_COL)[f\"preds_{model_name}\"]\n",
        "    tournament_data[f\"preds_{model_name}_neutral_riskiest_50\"] = neutralize(df=tournament_data,\n",
        "                                                                            columns=[f\"preds_{model_name}\"],\n",
        "                                                                            neutralizers=riskiest_features,\n",
        "                                                                            proportion=1.0,\n",
        "                                                                            normalize=True,\n",
        "                                                                            era_col=ERA_COL)[f\"preds_{model_name}\"]\n",
        "\n",
        "    pred_cols.add(f\"preds_{model_name}\")\n",
        "    pred_cols.add(f\"preds_{model_name}_neutral_riskiest_50\")\n",
        "\n",
        "\n",
        "# rank per era for each prediction column so that we can combine safely\n",
        "validation_data[list(pred_cols)] = validation_data.groupby(ERA_COL).apply(lambda d: d[list(pred_cols)].rank(pct=True))\n",
        "tournament_data[list(pred_cols)] = tournament_data.groupby(ERA_COL).apply(lambda d: d[list(pred_cols)].rank(pct=True))\n",
        "# make ensembles for val and tournament\n",
        "print('creating ensembles for tournament and validation')\n",
        "validation_data[\"ensemble_neutral_riskiest_50\"] = sum(\n",
        "    [validation_data[pred_col] for pred_col in pred_cols if pred_col.endswith(\"neutral_riskiest_50\")]).rank(\n",
        "    pct=True)\n",
        "tournament_data[\"ensemble_neutral_riskiest_50\"] = sum(\n",
        "    [tournament_data[pred_col] for pred_col in pred_cols if pred_col.endswith(\"neutral_riskiest_50\")]).rank(\n",
        "    pct=True)\n",
        "ensemble_cols.add(\"ensemble_neutral_riskiest_50\")\n",
        "\n",
        "validation_data[\"ensemble_not_neutral\"] = sum(\n",
        "    [validation_data[pred_col] for pred_col in pred_cols if \"neutral\" not in pred_col]).rank(pct=True)\n",
        "tournament_data[\"ensemble_not_neutral\"] = sum(\n",
        "    [tournament_data[pred_col] for pred_col in pred_cols if \"neutral\" not in pred_col]).rank(pct=True)\n",
        "ensemble_cols.add(\"ensemble_not_neutral\")\n",
        "\n",
        "validation_data[\"ensemble_all\"] = sum([validation_data[pred_col] for pred_col in pred_cols]).rank(pct=True)\n",
        "tournament_data[\"ensemble_all\"] = sum([tournament_data[pred_col] for pred_col in pred_cols]).rank(pct=True)\n",
        "ensemble_cols.add(\"ensemble_all\")\n",
        "\n",
        "gc.collect()\n",
        "print(\"getting final validation stats\")\n",
        "# get our final validation stats for our chosen model\n",
        "validation_stats = validation_metrics(validation_data, [best_pred_col], example_col=EXAMPLE_PREDS_COL,\n",
        "                                      fast_mode=False)\n",
        "print(validation_stats.to_markdown())\n",
        "\n",
        "# rename best model to prediction and rank from 0 to 1 to meet diagnostic/submission file requirements\n",
        "validation_data[\"prediction\"] = validation_data[best_pred_col].rank(pct=True)\n",
        "tournament_data[\"prediction\"] = tournament_data[best_pred_col].rank(pct=True)\n",
        "save_prediction(validation_data[\"prediction\"], f\"validation_predictions_{current_round}\")\n",
        "save_prediction(tournament_data[\"prediction\"], f\"tournament_predictions_{current_round}\")"
      ]
    }
  ]
}