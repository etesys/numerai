import matplotlib
import numpy as np
import pandas as pd
from numerapi import NumerAPI
import random
import sklearn
import lightgbm
import matplotlib.pyplot as plt
from utils import save_model, load_model, neutralize, get_biggest_change_features, validation_metrics, download_data
%matplotlib inline

from sklearn import (
    feature_extraction, feature_selection, decomposition, linear_model,
    model_selection, metrics, svm
)

#pandas.options.display.max_rows=1000
#pandas.options.display.max_columns=300
napi = NumerAPI()

current_round = napi.get_current_round(tournament=8)  # tournament 8 is the primary Numerai Tournament

# read in all of the new datas
# tournament data and example predictions change every week so we specify the round in their names
# training and validation data only change periodically, so no need to download them over again every single week
napi.download_dataset("numerai_training_data.parquet", "numerai_training_data.parquet")
df = pd.read_parquet('numerai_training_data.parquet')
df.head()
✔ Downloading numerai_training_data.parquet⠙ Downloading numerai_training_data.parquet⠹ Downloading numerai_training_data.parquet⠸ Downloading numerai_training_data.parquet⠼ Downloading numerai_training_data.parquet Downloading numerai_training_data.parquet
era	data_type	feature_dichasial_hammier_spawner	feature_rheumy_epistemic_prancer	feature_pert_performative_hormuz	feature_hillier_unpitied_theobromine	feature_perigean_bewitching_thruster	feature_renegade_undomestic_milord	feature_koranic_rude_corf	feature_demisable_expiring_millepede	...	target_paul_20	target_paul_60	target_george_20	target_george_60	target_william_20	target_william_60	target_arthur_20	target_arthur_60	target_thomas_20	target_thomas_60
id																					
n003bba8a98662e4	0001	train	1.0	0.50	1.00	1.00	0.00	0.00	1.00	1.00	...	0.25	0.25	0.25	0.00	0.166667	0.000000	0.166667	0.000000	0.166667	0.000000
n003bee128c2fcfc	0001	train	0.5	1.00	0.25	0.75	0.00	0.75	0.50	0.75	...	1.00	1.00	1.00	1.00	0.833333	0.666667	0.833333	0.666667	0.833333	0.666667
n0048ac83aff7194	0001	train	0.5	0.25	0.75	0.00	0.75	0.00	0.75	0.75	...	0.50	0.25	0.25	0.25	0.500000	0.333333	0.500000	0.333333	0.500000	0.333333
n00691bec80d3e02	0001	train	1.0	0.50	0.50	0.75	0.00	1.00	0.25	1.00	...	0.50	0.50	0.50	0.50	0.666667	0.500000	0.500000	0.500000	0.666667	0.500000
n00b8720a2fdc4f2	0001	train	1.0	0.75	1.00	1.00	0.00	0.00	1.00	0.50	...	0.50	0.50	0.50	0.50	0.666667	0.500000	0.500000	0.500000	0.666667	0.500000
5 rows × 1073 columns

# There are 2412105 rows grouped into 574 weekly eras
df.shape
(2412105, 1073)
# There's 1050 features with fun names generated by a hashing function
features = [c for c in df if c.startswith("feature")]
df["erano"] = df.era.astype(int)
eras = df.erano
target = "target"
print(len(features))
print(features[:5])
1050
['feature_dichasial_hammier_spawner', 'feature_rheumy_epistemic_prancer', 'feature_pert_performative_hormuz', 'feature_hillier_unpitied_theobromine', 'feature_perigean_bewitching_thruster']

There are now 20 targets!
There are 10 different types of targets constructed and 2 versions of each constructed with a 20 day window or a 60 day window
You are only scored on 'target' which currently corresponds to 'target_nomi_20' but could change in the future
The other auxillary targets can be very useful for training good models. Indeed you may find that a model trained on some of the auxillary targets generalize out-of-sample to 'target' better than a model trained on 'target'
target = "target"
targets = [c for c in df if c.startswith("target")]
len(targets)
21
# The targets have a wide range of correlations with each other from ~0.3 to ~0.9
# This should allow the construction of many diverse models which ensemble nicely
df[targets].corr()
target	target_nomi_20	target_nomi_60	target_jerome_20	target_jerome_60	target_janet_20	target_janet_60	target_ben_20	target_ben_60	target_alan_20	...	target_paul_20	target_paul_60	target_george_20	target_george_60	target_william_20	target_william_60	target_arthur_20	target_arthur_60	target_thomas_20	target_thomas_60
target	1.000000	1.000000	0.503147	0.769365	0.455448	0.659012	0.392255	0.819540	0.455063	0.678464	...	0.751827	0.441127	0.722665	0.401227	0.819724	0.484425	0.829850	0.490358	0.880602	0.486349
target_nomi_20	1.000000	1.000000	0.503147	0.769365	0.455448	0.659012	0.392255	0.819540	0.455063	0.678464	...	0.751827	0.441127	0.722665	0.401227	0.819724	0.484425	0.829850	0.490358	0.880602	0.486349
target_nomi_60	0.503147	0.503147	1.000000	0.433388	0.817732	0.373979	0.689912	0.456754	0.813261	0.384636	...	0.423820	0.791360	0.409423	0.705469	0.464266	0.873005	0.468459	0.879774	0.492558	0.877989
target_jerome_20	0.769365	0.769365	0.433388	1.000000	0.463756	0.698353	0.382269	0.768737	0.426895	0.628114	...	0.905954	0.446691	0.689335	0.379908	0.865786	0.453824	0.871040	0.457756	0.748965	0.421251
target_jerome_60	0.455448	0.455448	0.817732	0.463756	1.000000	0.382460	0.693168	0.450810	0.821748	0.372656	...	0.451384	0.884963	0.408582	0.719172	0.457941	0.862257	0.459407	0.865918	0.447718	0.789250
target_janet_20	0.659012	0.659012	0.373979	0.698353	0.382460	1.000000	0.422820	0.641273	0.358757	0.718364	...	0.703169	0.370593	0.576267	0.318308	0.724738	0.385911	0.747625	0.395341	0.648114	0.364063
target_janet_60	0.392255	0.392255	0.689912	0.382269	0.693168	0.422820	1.000000	0.378579	0.672070	0.414573	...	0.374259	0.695526	0.341069	0.596787	0.388464	0.720954	0.395379	0.742915	0.386067	0.677794
target_ben_20	0.819540	0.819540	0.456754	0.768737	0.450810	0.641273	0.378579	1.000000	0.485953	0.655087	...	0.745765	0.433137	0.757761	0.418228	0.746513	0.445281	0.755789	0.449744	0.872847	0.476992
target_ben_60	0.455063	0.455063	0.813261	0.426895	0.821748	0.358757	0.672070	0.485953	1.000000	0.373185	...	0.416077	0.786051	0.425653	0.747294	0.424105	0.787089	0.427728	0.795083	0.480622	0.867900
target_alan_20	0.678464	0.678464	0.384636	0.628114	0.372656	0.718364	0.414573	0.655087	0.373185	1.000000	...	0.614438	0.361734	0.581200	0.328936	0.642762	0.381904	0.661255	0.392130	0.702886	0.385541
target_alan_60	0.382948	0.382948	0.673598	0.353125	0.659323	0.392251	0.769956	0.373347	0.646430	0.419483	...	0.346406	0.640502	0.335088	0.568619	0.363717	0.677427	0.372271	0.696971	0.388124	0.695771
target_paul_20	0.751827	0.751827	0.423820	0.905954	0.451384	0.703169	0.374259	0.745765	0.416077	0.614438	...	1.000000	0.473859	0.696137	0.399401	0.847479	0.445068	0.840886	0.447869	0.730521	0.412473
target_paul_60	0.441127	0.441127	0.791360	0.446691	0.884963	0.370593	0.695526	0.433137	0.786051	0.361734	...	0.473859	1.000000	0.417146	0.741728	0.444607	0.835873	0.444850	0.826746	0.432580	0.763650
target_george_20	0.722665	0.722665	0.409423	0.689335	0.408582	0.576267	0.341069	0.757761	0.425653	0.581200	...	0.696137	0.417146	1.000000	0.482188	0.676223	0.406019	0.683647	0.409641	0.731028	0.417317
target_george_60	0.401227	0.401227	0.705469	0.379908	0.719172	0.318308	0.596787	0.418228	0.747294	0.328936	...	0.399401	0.741728	0.482188	1.000000	0.380864	0.701942	0.382972	0.707987	0.413474	0.718592
target_william_20	0.819724	0.819724	0.464266	0.865786	0.457941	0.724738	0.388464	0.746513	0.424105	0.642762	...	0.847479	0.444607	0.676223	0.380864	1.000000	0.497816	0.934503	0.490892	0.801328	0.452694
target_william_60	0.484425	0.484425	0.873005	0.453824	0.862257	0.385911	0.720954	0.445281	0.787089	0.381904	...	0.445068	0.835873	0.406019	0.701942	0.497816	1.000000	0.488486	0.934996	0.479017	0.848806
target_arthur_20	0.829850	0.829850	0.468459	0.871040	0.459407	0.747625	0.395379	0.755789	0.427728	0.661255	...	0.840886	0.444850	0.683647	0.382972	0.934503	0.488486	1.000000	0.496131	0.816706	0.459133
target_arthur_60	0.490358	0.490358	0.879774	0.457756	0.865918	0.395341	0.742915	0.449744	0.795083	0.392130	...	0.447869	0.826746	0.409641	0.707987	0.490892	0.934996	0.496131	1.000000	0.486803	0.863319
target_thomas_20	0.880602	0.880602	0.492558	0.748965	0.447718	0.648114	0.386067	0.872847	0.480622	0.702886	...	0.730521	0.432580	0.731028	0.413474	0.801328	0.479017	0.816706	0.486803	1.000000	0.519180
target_thomas_60	0.486349	0.486349	0.877989	0.421251	0.789250	0.364063	0.677794	0.476992	0.867900	0.385541	...	0.412473	0.763650	0.417317	0.718592	0.452694	0.848806	0.459133	0.863319	0.519180	1.000000
21 rows × 21 columns

1050 features, no feature groups!
# The features are no longer explicit feature groups, but you can see some pretty obvious patterns if you
# visualize the feature correlation matrix. Feel free to construct your own groupings!
plt.figure(figsize = (8,8))
plt.imshow(df[df.era=='0001'][features].corr())
<matplotlib.image.AxesImage at 0x7f2f446a54f0>

# The models should be scored based on the rank-correlation (spearman) with the target
def numerai_score(y_true, y_pred):
    rank_pred = y_pred.groupby(eras).apply(lambda x: x.rank(pct=True, method="first"))
    return np.corrcoef(y_true, rank_pred)[0,1]

# It can also be convenient while working to evaluate based on the regular (pearson) correlation
def correlation_score(y_true, y_pred):
    return np.corrcoef(y_true, y_pred)[0,1]
# There are 574 eras numbered from 1 to 574
eras.describe()
count    2.412105e+06
mean     3.043023e+02
std      1.598982e+02
min      1.000000e+00
25%      1.720000e+02
50%      3.040000e+02
75%      4.430000e+02
max      5.740000e+02
Name: erano, dtype: float64
# The earlier eras are smaller, but generally each era is 4000-5000 rows
df.groupby(eras).size().plot()
<AxesSubplot:xlabel='erano'>

# The target is discrete and takes on 5 different values with about 5% in 0 & 1, 20% in 0.75 & 0.25, and 50% in 0.50
df.groupby(target).size()/len(df)
target
0.00    0.050003
0.25    0.199996
0.50    0.499993
0.75    0.200015
1.00    0.049992
dtype: float64
# Some new targets have a different distribution and the bins are less rigid in the percent of samples contained
print(df.groupby('target_william_20').size()/len(df))
print()
print(df.groupby('target_arthur_20').size()/len(df))
target_william_20
0.000000    0.029735
0.166667    0.059873
0.333333    0.211150
0.500000    0.391798
0.666667    0.203550
0.833333    0.065247
1.000000    0.038511
dtype: float64

target_arthur_20
0.000000    0.028859
0.166667    0.073267
0.333333    0.258668
0.500000    0.384931
0.666667    0.173287
0.833333    0.052486
1.000000    0.028367
dtype: float64

Some of the features are very correlated
Some even perfectly correlated within a particular era

feature_corrs = df[df.era=='0001'][features].corr()
feature_corrs.stack().head()
feature_dichasial_hammier_spawner  feature_dichasial_hammier_spawner       1.000000
                                   feature_rheumy_epistemic_prancer        0.001690
                                   feature_pert_performative_hormuz        0.141253
                                   feature_hillier_unpitied_theobromine    0.170891
                                   feature_perigean_bewitching_thruster   -0.151098
dtype: float64
tdf = feature_corrs.stack()
tdf = tdf[tdf.index.get_level_values(0) < tdf.index.get_level_values(1)]
tdf.sort_values()
feature_calycled_living_birmingham            feature_dentilingual_removed_osmometer             -0.947381
feature_criticisable_authentical_deprecation  feature_invalid_extortionary_titillation           -0.927235
feature_hierologic_assault_candida            feature_multicostate_undescended_monoacid          -0.926623
feature_apogamic_obeisant_pondicherry         feature_splitting_sexagesimal_teutonisation        -0.923430
feature_earthlier_parian_pistachio            feature_grandmotherly_circumnavigable_homonymity   -0.913526
                                                                                                    ...   
feature_escutcheoned_timocratic_kotwal        feature_horizontal_snug_description                 1.000000
                                              feature_neptunian_supersensitive_stapler            1.000000
                                              feature_interoceptive_fifteenth_trey                1.000000
                                              feature_savory_engrained_undervest                  1.000000
feature_brittle_chautauqua_elite              feature_steepled_transistorized_shelter             1.000000
Length: 550725, dtype: float64
The correlation can change over time
You can see this by comparing feature correlations on the first half and second half on the training set

df1 = df[eras<=eras.median()]
df2 = df[eras>eras.median()]
corr1 = df1[df1.era=='0001'][features].corr().unstack()
corr1 = corr1[corr1.index.get_level_values(0) < corr1.index.get_level_values(1)]
corr2 = df2[df2.era=='0574'][features].corr().unstack()
corr2 = corr2[corr2.index.get_level_values(0) < corr2.index.get_level_values(1)]
tdf = pd.DataFrame({
    "corr1": corr1,
    "corr2": corr2,
})
tdf["corr_diff"] = tdf.corr2 - tdf.corr1
tdf.sort_values(by="corr_diff")
corr1	corr2	corr_diff
feature_brushed_psychiatrical_lubbock	feature_savory_engrained_undervest	1.000000	-0.159888	-1.159888
feature_interpenetrative_enabling_elm	feature_savory_engrained_undervest	1.000000	-0.159262	-1.159262
feature_horizontal_snug_description	feature_savory_engrained_undervest	1.000000	-0.158691	-1.158691
feature_abstersive_emotional_misinterpreter	feature_horizontal_snug_description	1.000000	-0.156998	-1.156998
feature_opposable_argumentative_mesoblast	feature_savory_engrained_undervest	1.000000	-0.156654	-1.156654
...	...	...	...	...
feature_descendent_decanal_hon	feature_petitionary_evanescent_diallage	0.128122	0.944659	0.816537
feature_indirect_concrete_canaille	feature_juvenalian_paunchy_uniformitarianism	-0.098432	0.727993	0.826425
feature_draconic_contractible_romper	feature_indirect_concrete_canaille	-0.098432	0.737243	0.835675
feature_bleeding_arabesque_pneuma	feature_instrumentalist_extrovert_cassini	-0.135409	0.720442	0.855851
feature_juvenalian_paunchy_uniformitarianism	feature_lipogrammatic_blowsier_seismometry	-0.089265	0.783615	0.872879
550725 rows × 3 columns

Some features are predictive on their own
feature_scores = {feature: score for feature, score in zip(features, np.corrcoef(df[df.era=='0001'][[target]+features].T)[1:,0])}
pd.Series(feature_scores).sort_values()
feature_lofty_acceptable_challenge           -0.082876
feature_unvaried_social_bangkok              -0.082876
feature_undivorced_unsatisfying_praetorium   -0.072182
feature_literate_treacly_altercation         -0.070247
feature_commensurable_industrial_jungfrau    -0.069840
                                                ...   
feature_apomictical_motorized_vaporisation    0.055358
feature_delimited_doable_jim                  0.056142
feature_durative_cuboidal_mirepoix            0.056497
feature_airborne_genitival_cathouse           0.058023
feature_alloyed_copyright_protium             0.059169
Length: 1050, dtype: float64
# Single features do not work consistently though
by_era_correlation = pd.Series({
    era: np.corrcoef(tdf[target], tdf["feature_grandmotherly_circumnavigable_homonymity"])[0,1]
    for era, tdf in df.groupby(eras)
})
by_era_correlation.plot()
<AxesSubplot:>

# With a rolling 10 era average you can see some trends
by_era_correlation.rolling(10).mean().plot()
<AxesSubplot:>

The feature exposures of different targets can be very different, even orthogonal
feature_scores_nomi20 = {feature: score for feature, score in zip(features, np.corrcoef(df[df.era=='0001'][['target_nomi_20']+features].T)[1:,0])}
feature_scores_nomi60 = {feature: score for feature, score in zip(features, np.corrcoef(df[df.era=='0001'][['target_nomi_60']+features].fillna(0.5).T)[1:,0])}
plt.scatter(feature_scores_nomi20.values(), feature_scores_nomi60.values())
print(np.corrcoef(list(feature_scores_nomi20.values()), list(feature_scores_nomi60.values()))[0,1])
0.6713159328865955

feature_scores_jerome = {feature: score for feature, score in zip(features, np.corrcoef(df[df.era=='0001'][['target_jerome_20']+features].T)[1:,0])}
feature_scores_janet = {feature: score for feature, score in zip(features, np.corrcoef(df[df.era=='0001'][['target_janet_20']+features].T)[1:,0])}
plt.scatter(feature_scores_jerome.values(), feature_scores_janet.values())
print(np.corrcoef(list(feature_scores_jerome.values()), list(feature_scores_janet.values()))[0,1])
0.05603328047427541

Gotcha: MSE looks worse than correlation out of sample
Models will generally be overconfident, so even if they are good at ranking rows, the Mean-Squared-Error of the residuals could be larger than even the Mean-Squared-Error of the target (r-squared<0)

df1 = df[eras<=eras.median()]
df2 = df[eras>eras.median()]
linear1 = linear_model.LinearRegression()
linear1.fit(df1[features], df1[target])
linear2 = linear_model.LinearRegression()
linear2.fit(df2[features], df2[target])
LinearRegression()
# Note in particular that the R-squared of (train_on_1, eval_on_2) is slightly negative!
r2 = [
    [
        model.score(dfX[features], dfX[target])
        for dfX in [df1, df2]
    ]
    for model in [linear1, linear2]
]
pd.DataFrame(r2, columns=["eval_on_1", "eval_on_2"], index=["train_on_1", "train_on_2"])
eval_on_1	eval_on_2
train_on_1	0.007108	-0.000476
train_on_2	-0.000780	0.006694
# Note in particular that the correlation of (train_on_1, eval_on_2) is quite decent
corrs = [
    [
        numerai_score(dfX[target], pd.Series(model.predict(dfX[features]), index=dfX.index))
        for dfX in [df1, df2]
    ]
    for model in [linear1, linear2]
]
pd.DataFrame(corrs, columns=["eval_on_1", "eval_on_2"], index=["train_on_1", "train_on_2"])
eval_on_1	eval_on_2
train_on_1	0.081889	0.037630
train_on_2	0.038557	0.078762
# This can be be run with LightGBM as well
lgb1 = lightgbm.LGBMRegressor()
lgb1.fit(df1[features], df1[target])
lgb2 = lightgbm.LGBMRegressor()
lgb2.fit(df2[features], df2[target])
LGBMRegressor()
r2 = [
    [
        model.score(dfX[features], dfX[target])
        for dfX in [df1, df2]
    ]
    for model in [lgb1, lgb2]
]
pd.DataFrame(r2, columns=["eval_on_1", "eval_on_2"], index=["train_on_1", "train_on_2"])
eval_on_1	eval_on_2
train_on_1	0.025232	0.002172
train_on_2	0.002142	0.025901
corrs = [
    [
        numerai_score(dfX[target], pd.Series(model.predict(dfX[features]), index=dfX.index))
        for dfX in [df1, df2]
    ]
    for model in [lgb1, lgb2]
]
pd.DataFrame(corrs, columns=["eval_on_1", "eval_on_2"], index=["train_on_1", "train_on_2"])
eval_on_1	eval_on_2
train_on_1	0.192827	0.047101
train_on_2	0.048331	0.197548
Gotcha: Eras are now overlapping!
Eras are weekly (5 days), but the target is four weeks out (20 days). You can subsample the eras (every 4th era) to prevent models from overfitting due to overlapping eras. You can then ensemble the models trained on each of the 4 different ways to subsample.

# train model on all eras in df1
lgb_all = lightgbm.LGBMRegressor()
lgb_all.fit(df1[features], df1[target])
LGBMRegressor()
lgb_all.feature_name_
['feature_dichasial_hammier_spawner',
 'feature_rheumy_epistemic_prancer',
 'feature_pert_performative_hormuz',
 'feature_hillier_unpitied_theobromine',
 'feature_perigean_bewitching_thruster',
 'feature_renegade_undomestic_milord',
 'feature_koranic_rude_corf',
 'feature_demisable_expiring_millepede',
 'feature_unscheduled_malignant_shingling',
 'feature_clawed_unwept_adaptability',
 'feature_rubblier_chlorotic_stogy',
 'feature_untumbled_histologic_inion',
 'feature_piffling_inflamed_jupiter',
 'feature_abstersive_emotional_misinterpreter',
 'feature_unluckiest_mulley_benzyl',
 'feature_escutcheoned_timocratic_kotwal',
 'feature_integrated_extroversive_ambivalence',
 'feature_vedic_mitral_swiz',
 'feature_reclaimed_fallibilist_turpentine',
 'feature_gone_honduran_worshipper',
 'feature_insociable_exultant_tatum',
 'feature_outdated_tapered_speciation',
 'feature_leggiest_slaggiest_inez',
 'feature_chaldean_vixenly_propylite',
 'feature_hysteric_mechanized_recklinghausen',
 'feature_glare_factional_assessment',
 'feature_highland_eocene_berean',
 'feature_seemlier_reorient_monandry',
 'feature_expressed_abhominable_pruning',
 'feature_castrated_presented_quizzer',
 'feature_restricted_aggregately_workmanship',
 'feature_scorbutic_intellectualism_mongoloid',
 'feature_telephonic_shakable_bollock',
 'feature_subglobular_unsalable_patzer',
 'feature_syrian_coital_counterproof',
 'feature_supergene_legible_antarthritic',
 'feature_hypothetic_distressing_endemic',
 'feature_torturesome_estimable_preferrer',
 'feature_greedier_favorable_enthymeme',
 'feature_decent_solo_stickup',
 'feature_unmalleable_resistant_kingston',
 'feature_seamier_jansenism_inflator',
 'feature_lordly_lamellicorn_buxtehude',
 'feature_mattery_past_moro',
 'feature_helpable_chanciest_fractionisation',
 'feature_spookiest_expedite_overnighter',
 'feature_culinary_pro_offering',
 'feature_ganoid_osiered_mineralogy',
 'feature_hotfoot_behaviorist_terylene',
 'feature_severe_tricky_pinochle',
 'feature_maximal_unobserving_desalinisation',
 'feature_voltairean_consolidative_parallel',
 'feature_unmoved_alt_spoonerism',
 'feature_tittering_virgilian_decliner',
 'feature_methylated_necrophilic_serendipity',
 'feature_calceolate_pudgy_armure',
 'feature_unsparing_moralistic_commissary',
 'feature_criticisable_authentical_deprecation',
 'feature_undetermined_idle_aftergrowth',
 'feature_antipathetical_terrorful_ife',
 'feature_caressive_cognate_cubature',
 'feature_crablike_panniered_gloating',
 'feature_exacerbating_presentationism_apagoge',
 'feature_epicurean_fetal_seising',
 'feature_casuistic_barbarian_monochromy',
 'feature_haematoid_runaway_nightjar',
 'feature_croupiest_shaded_thermotropism',
 'feature_zarathustrian_albigensian_itch',
 'feature_unsparred_scarabaeid_anthologist',
 'feature_unco_terefah_thirster',
 'feature_sallowish_cognisant_romaunt',
 'feature_grazed_blameful_desiderative',
 'feature_whitened_remanent_blast',
 'feature_introvert_symphysial_assegai',
 'feature_casemated_ibsenian_grantee',
 'feature_intertwined_leeriest_suffragette',
 'feature_built_reincarnate_sherbet',
 'feature_axillary_reluctant_shorty',
 'feature_descendent_decanal_hon',
 'feature_untrimmed_monaxial_accompanist',
 'feature_desiderative_commiserative_epizoa',
 'feature_subdued_spiffier_kano',
 'feature_affricative_bromic_raftsman',
 'feature_doggish_whacking_headscarf',
 'feature_congenital_conched_perithecium',
 'feature_terrific_epigamic_affectivity',
 'feature_enlightening_mirthful_laurencin',
 'feature_continuate_unprocurable_haversine',
 'feature_fustiest_voiced_janet',
 'feature_ovular_powered_neckar',
 'feature_questionable_diplex_caesarist',
 'feature_nubblier_plosive_deepening',
 'feature_revitalizing_dashing_photomultiplier',
 'feature_chafed_undenominational_backstitch',
 'feature_assenting_darn_arthropod',
 'feature_consecrate_legislative_cavitation',
 'feature_inhabited_pettier_veinlet',
 'feature_voltairean_dyslogistic_epagoge',
 'feature_log_unregenerate_babel',
 'feature_agile_unrespited_gaucho',
 'feature_whopping_eminent_attempter',
 'feature_travelled_semipermeable_perruquier',
 'feature_horizontal_snug_description',
 'feature_inhibited_snowiest_drawing',
 'feature_salian_suggested_ephemeron',
 'feature_diverticular_punjabi_matronship',
 'feature_reminiscent_unpained_ukulele',
 'feature_unsurveyed_chopped_feldspathoid',
 'feature_degenerate_officinal_feasibility',
 'feature_undrossy_serpentiform_sack',
 'feature_festering_controvertible_hostler',
 'feature_leukemic_paler_millikan',
 'feature_subapostolic_dungy_fermion',
 'feature_wale_planned_tolstoy',
 'feature_bifacial_hexastyle_hemialgia',
 'feature_revealable_aeonian_elvira',
 'feature_perceivable_gasiform_psammite',
 'feature_smugger_hydroponic_farnesol',
 'feature_apostate_impercipient_knighthood',
 'feature_unperfect_implemental_cellarage',
 'feature_stereotypic_ebracteate_louise',
 'feature_offshore_defamatory_catalog',
 'feature_tribal_germinable_yarraman',
 'feature_dispiriting_araeostyle_jersey',
 'feature_clerkish_flowing_chapati',
 'feature_venatic_intermetallic_darling',
 'feature_nebule_barmier_bibliomania',
 'feature_unknown_reusable_cabbage',
 'feature_planar_unessential_bride',
 'feature_irritant_euphuistic_weka',
 'feature_whistleable_unbedimmed_chokey',
 'feature_roiling_trimeric_kurosawa',
 'feature_loyal_fishy_pith',
 'feature_unlivable_morbific_traveling',
 'feature_untellable_penal_allegorization',
 'feature_coordinated_undecipherable_gag',
 'feature_congealed_lee_steek',
 'feature_intrusive_effluent_hokkaido',
 'feature_epidermic_scruffiest_prosperity',
 'feature_quadratic_untouched_liberty',
 'feature_hypogastric_effectual_sunlight',
 'feature_contused_festal_geochemistry',
 'feature_hexametric_ventricose_limnology',
 'feature_paramagnetic_complex_gish',
 'feature_chuffier_analectic_conchiolin',
 'feature_indefatigable_enterprising_calf',
 'feature_untouchable_unsolvable_agouti',
 'feature_reduplicate_conoid_albite',
 'feature_sixteen_inbreed_are',
 'feature_dovetailed_winy_hanaper',
 'feature_bushwhacking_unaligned_imperturbability',
 'feature_undescribed_methylic_friday',
 'feature_inflexed_lamaism_crit',
 'feature_calefactive_anapaestic_jerome',
 'feature_agronomic_cryptal_advisor',
 'feature_unbreakable_nosological_comedian',
 'feature_unmodernized_vasodilator_galenist',
 'feature_paraffinoid_irreplevisable_ombu',
 'feature_invalid_extortionary_titillation',
 'feature_fake_trident_agitator',
 'feature_wieldable_defiled_aperitive',
 'feature_contaminative_intrusive_tagrag',
 'feature_himyarite_tetragonal_deceit',
 'feature_tossing_denominative_threshing',
 'feature_dendritic_prothallium_sweeper',
 'feature_ruffianly_uncommercial_anatole',
 'feature_hellenistic_scraggly_comfort',
 'feature_squishiest_unsectarian_support',
 'feature_myographic_gawkier_timbale',
 'feature_faltering_tergal_tip',
 'feature_antisubmarine_foregoing_cryosurgery',
 'feature_altern_unnoticed_impregnation',
 'feature_foamy_undrilled_glaciology',
 'feature_appraisive_anagrammatical_tentacle',
 'feature_emmetropic_heraclitean_conducting',
 'feature_collective_stigmatic_handfasting',
 'feature_iridic_unpropertied_spline',
 'feature_farcical_spinal_samantha',
 'feature_urochordal_swallowed_curn',
 'feature_wombed_reverberatory_colourer',
 'feature_merovingian_tenebrism_hartshorn',
 'feature_stylistic_honduran_comprador',
 'feature_caecilian_unexperienced_ova',
 'feature_fumarolic_known_sharkskin',
 'feature_belgravian_salopian_sheugh',
 'feature_zymotic_varnished_mulga',
 'feature_learned_claustral_quiddity',
 'feature_brickier_heterostyled_scrutiny',
 'feature_unnetted_bay_premillennialist',
 'feature_uncurtailed_translucid_coccid',
 'feature_jiggish_tritheist_probity',
 'feature_groggy_undescried_geosphere',
 'feature_hawkish_domiciliary_duramen',
 'feature_base_ingrain_calligrapher',
 'feature_accessorial_aroused_crochet',
 'feature_fierier_goofier_follicle',
 'feature_unburied_exponent_pace',
 'feature_chelonian_pyknic_delphi',
 'feature_tarry_meet_chapel',
 'feature_generative_honorific_tughrik',
 'feature_smoggy_niftiest_lunch',
 'feature_coalier_typhoid_muntin',
 'feature_chopfallen_fasciate_orchidologist',
 'feature_euterpean_frazzled_williamsburg',
 'feature_quinsied_increased_braincase',
 'feature_unlawful_superintendent_brunet',
 'feature_naval_edified_decarbonize',
 'feature_scenic_cormophytic_bilirubin',
 'feature_unvaried_social_bangkok',
 'feature_fissirostral_multifoliate_chillon',
 'feature_citified_certified_progestin',
 'feature_otherworldly_hallstatt_tolerator',
 'feature_unawakening_escapism_totemist',
 'feature_chillier_consultatory_latest',
 'feature_fustier_crossing_pharyngology',
 'feature_nobiliary_nativist_pull',
 'feature_hieroglyphic_scalar_synchroflash',
 'feature_tricksy_dithyrambic_pallet',
 'feature_unfearing_phraseological_adder',
 'feature_symphysial_cossack_bonaparte',
 'feature_dextrogyrate_permitted_mystagogy',
 'feature_interfertile_neozoic_solder',
 'feature_theocratic_discriminate_subchloride',
 'feature_neptunian_supersensitive_stapler',
 'feature_auroral_glary_quamash',
 'feature_interoceptive_fifteenth_trey',
 'feature_embryo_sloping_albertine',
 'feature_beery_somatologic_elimination',
 'feature_gustier_maladjusted_bargeman',
 'feature_septarian_vapory_thatching',
 'feature_seismographic_matronly_dimeter',
 'feature_burning_incunabular_clodhopper',
 'feature_poachier_unsufferable_bile',
 'feature_incidental_unveiled_bobbysoxer',
 'feature_prevailing_thrifty_organicism',
 "feature_dirtiest_possessory_nuku'alofa",
 'feature_chinked_preborn_hideaway',
 'feature_caudated_judicial_crossette',
 'feature_hyperbaric_populated_optimum',
 'feature_petalled_backmost_crimplene',
 'feature_capitate_plaguy_hooper',
 'feature_faced_moline_gavotte',
 'feature_russety_boustrophedon_dactylogram',
 'feature_inherited_accelerated_astaire',
 'feature_waving_unmourned_bloke',
 'feature_sidearm_pitchiest_line',
 'feature_eating_lanose_linguini',
 'feature_foliated_psychokinetic_vivian',
 'feature_emissive_perinephric_conveniency',
 'feature_unforced_atwitter_nailer',
 'feature_provisionary_unfostered_subadar',
 'feature_liverpudlian_multicellular_torsk',
 'feature_frowzy_berserk_lag',
 'feature_screwy_asianic_balanchine',
 'feature_unmeaning_ungyved_outboard',
 'feature_unexceptionable_saprozoic_culturist',
 'feature_unheaded_unmated_behaviour',
 'feature_unblunted_titubant_sequoia',
 'feature_perturbed_dry_grosvenor',
 'feature_mortal_sovran_powerboat',
 'feature_geodic_corky_clinic',
 'feature_latin_throwback_lactoscope',
 'feature_spicier_caller_billfold',
 'feature_bathypelagic_companionate_disenchanter',
 'feature_adept_unsensitive_hieroglyph',
 'feature_hotshot_undefied_aten',
 'feature_innumerate_alated_soliped',
 'feature_apogamic_obeisant_pondicherry',
 'feature_shabbiest_bewildering_editorialization',
 'feature_irreplaceable_regurgitate_ctenophore',
 'feature_socialized_obconical_itemization',
 'feature_amphictyonic_secretarial_induline',
 'feature_unsatisfied_mesopotamian_woodruff',
 'feature_backswept_cryptogenic_baku',
 'feature_healthiest_glial_cliquism',
 'feature_absent_herpetological_genitor',
 'feature_accountable_ethnographic_counterexample',
 'feature_unseparable_cedarn_orcadian',
 'feature_inhabited_collectable_soda',
 'feature_arizonan_endurable_daw',
 'feature_tanagrine_balconied_shut',
 'feature_powdered_incased_project',
 'feature_unassimilated_waxed_vesicle',
 'feature_conjunctional_maned_banian',
 'feature_contorted_weaponed_piaffe',
 'feature_leadiest_unliveable_macadamia',
 'feature_ignored_splay_management',
 'feature_aggrieved_rammish_tommy',
 'feature_coelomate_orbiculate_osteology',
 'feature_amoebic_scarabaeid_gypsum',
 'feature_unforbidden_flaming_settlement',
 'feature_edge_hanoverian_cockneyfication',
 'feature_picked_inferrible_lanark',
 'feature_geographical_ecstatic_geta',
 'feature_gymnastic_neap_thoracostomy',
 'feature_nonclinical_stewed_disreputability',
 'feature_tensed_cinnamonic_league',
 'feature_pongid_agentive_rhomboid',
 'feature_alloyed_copyright_protium',
 'feature_afloat_brickiest_supernationalism',
 'feature_healthful_mitigated_magnesium',
 'feature_hypertrophied_embryologic_forfeiter',
 'feature_few_lagoonal_alcyonarian',
 'feature_temporary_dreamful_aldershot',
 'feature_midnightly_falser_replacement',
 'feature_nomothetic_autistic_ilk',
 'feature_tyrian_wretched_frangipane',
 'feature_waxiest_orthogonal_hiroshima',
 'feature_raglan_gouty_twiddler',
 'feature_cleanly_kickable_miriam',
 'feature_necrophiliac_unsublimated_rudd',
 'feature_delimited_doable_jim',
 'feature_brushed_psychiatrical_lubbock',
 'feature_exsanguine_ethereal_archon',
 'feature_evaporative_largo_aster',
 'feature_incendiary_failing_fluff',
 'feature_aloetic_presented_scarer',
 'feature_ruly_comtist_passerine',
 'feature_gymnasial_supremacist_osteopathy',
 'feature_protolithic_ontogenic_fusibility',
 'feature_representable_ungored_zoffany',
 'feature_curvier_antinodal_absolute',
 'feature_zionist_enterable_vagabond',
 'feature_elegiac_isolationism_lookout',
 'feature_toilful_exordial_synchro',
 'feature_auxetic_cholagogue_sexton',
 'feature_inane_ribald_adrenal',
 'feature_sere_hateable_conspicuity',
 'feature_capsulate_rival_marcel',
 'feature_pressed_animalcular_garden',
 'feature_scythian_supposed_fortalice',
 'feature_elating_apeak_hoodwinker',
 'feature_telangiectatic_unwon_machtpolitik',
 'feature_quaggy_constabulary_mismanagement',
 'feature_intersidereal_chewy_sinusoid',
 'feature_yearning_tippy_taking',
 'feature_healthy_preterhuman_conto',
 'feature_wispy_reverberating_fyrd',
 'feature_inebriant_unhopeful_beecham',
 'feature_limbic_pulverizable_hindfoot',
 'feature_avowed_cloying_anthropologist',
 'feature_deformed_unenslaved_graniteware',
 'feature_foudroyant_undersigned_ascendance',
 'feature_aberrant_frustrate_drosophila',
 'feature_loved_purpure_malthusian',
 'feature_unsyllabled_skin_underworkman',
 'feature_catercorner_seemliest_prepostor',
 'feature_proletary_nonionic_varment',
 'feature_concessible_carpal_telesthesia',
 'feature_acquirable_helvetic_tercel',
 'feature_adynamic_glossier_ghana',
 'feature_frizzier_alcoholic_gadolinium',
 'feature_dyeline_catechistic_placet',
 'feature_unaltered_squeezable_subway',
 'feature_eyed_scotistic_diabolism',
 'feature_imperviable_surprised_running',
 'feature_overfraught_chinked_jinx',
 'feature_nonfunctional_cnemial_illocution',
 'feature_sunk_pyrrho_allah',
 'feature_marxian_subminiature_minx',
 'feature_telling_collinear_estanciero',
 'feature_battled_beating_radiotelephony',
 'feature_messy_glottic_ciborium',
 'feature_adducent_ailing_gene',
 'feature_inoperable_monegasque_infuriation',
 'feature_suggestive_sexological_massasauga',
 'feature_radiculose_perfectionistic_swing',
 'feature_deliquescent_lintiest_protamine',
 'feature_splitting_sexagesimal_teutonisation',
 'feature_jauntier_talc_noreen',
 'feature_yankee_nonary_okavango',
 'feature_divulsive_cordial_bremsstrahlung',
 'feature_widespread_mannish_seaquake',
 'feature_registered_arrayed_galactagogue',
 'feature_dropping_naevoid_swag',
 'feature_unmown_circumventive_drayage',
 'feature_inelastic_verist_graf',
 'feature_streamlined_constricting_boz',
 'feature_sole_soapy_tangible',
 'feature_gnathic_terminological_defector',
 'feature_bucktooth_stockish_sideropenia',
 'feature_affable_yellow_terne',
 'feature_bedded_neutral_relishing',
 'feature_olid_reunionistic_malachi',
 'feature_imitable_artificial_titration',
 'feature_viverrine_lockable_quitch',
 'feature_mephitic_televisionary_breeder',
 'feature_talcose_patelliform_cana',
 'feature_muddier_besieged_escarp',
 'feature_protanopic_reptant_nimiety',
 'feature_wicker_crescent_slavocrat',
 'feature_fictile_shadowy_esse',
 'feature_guttering_liberating_conurbation',
 'feature_transpiring_subsolar_biggy',
 'feature_rascal_unprintable_lansing',
 'feature_carnassial_undecided_suspensor',
 'feature_unkindled_apterygial_contributory',
 'feature_equipotent_undefeated_anointer',
 'feature_gregorian_electromagnetic_hepar',
 'feature_dyspeptic_disseminative_anglophobia',
 'feature_durative_cuboidal_mirepoix',
 'feature_gyral_extrapolated_serotherapy',
 'feature_literary_live_taxon',
 'feature_deliberative_connatural_kinetoscope',
 'feature_dozier_mad_outrush',
 'feature_polybasic_unbreathed_magdeburg',
 'feature_attainable_solonian_impropriator',
 'feature_underlying_pegmatitic_disperser',
 'feature_counter_arbitral_gyration',
 'feature_biased_puzzling_cessation',
 'feature_stylar_denudate_intonation',
 'feature_bold_brindle_playground',
 'feature_brachydactylic_azonic_strife',
 'feature_saving_inhabitable_lighthouse',
 'feature_totemic_unshaded_encore',
 'feature_echoic_dustproof_indiscernibility',
 'feature_passable_premed_independent',
 'feature_dreary_penal_acroterium',
 'feature_haziest_lifelike_horseback',
 'feature_prevailing_nubile_hearst',
 'feature_iraqi_gentled_hector',
 'feature_theban_byronic_prolation',
 'feature_resolutive_betrothed_recession',
 'feature_motherly_chalkier_woking',
 'feature_untinged_crystallographic_succulent',
 'feature_plutonian_unsegregated_meringue',
 'feature_myological_adducent_puebla',
 'feature_cromwellian_imbecilic_algonkian',
 'feature_exterminated_grumbling_lawing',
 'feature_bhutan_imagism_dolerite',
 'feature_overt_smart_stendhal',
 'feature_bipinnate_oriented_earmuff',
 'feature_telial_gnarliest_darmstadt',
 'feature_savory_engrained_undervest',
 'feature_vibrational_schooled_quadrivalence',
 'feature_steepled_transistorized_shelter',
 'feature_heliocentric_stealthy_waning',
 'feature_aberdeen_tangled_providence',
 'feature_quarrelsome_sneakiest_misadventure',
 'feature_haunted_fussier_vaunt',
 'feature_astronomic_gripple_culverin',
 'feature_reproving_aurorean_parlour',
 'feature_baldish_cognitional_naha',
 'feature_grummest_confounding_quinquagesima',
 'feature_frowsiest_bonapartean_income',
 'feature_subcordate_hypsometric_matriarchate',
 'feature_cymose_prima_guardhouse',
 'feature_deceased_peppiest_hitlerism',
 'feature_unstitched_peppercorny_value',
 'feature_slack_calefacient_tableau',
 'feature_mesoblastic_anatolian_deodand',
 'feature_undissociated_fuzziest_monoamine',
 'feature_whitewashed_aliunde_mesopotamia',
 'feature_unsolvable_majuscular_caique',
 'feature_unwinding_prettyish_megger',
 'feature_estuarine_candescent_podunk',
 'feature_plaguy_preceding_hosteller',
 'feature_septic_cuticular_cape',
 'feature_fertilized_quartile_roll',
 'feature_elating_diluvian_telepathist',
 'feature_ginger_unexplored_sadduceeism',
 'feature_satanic_hominoid_lingo',
 'feature_bristly_encysted_vendue',
 'feature_semibold_disparaging_turnip',
 'feature_scythian_lithographical_whigmaleerie',
 'feature_store_apteral_isocheim',
 'feature_overmerry_quinary_raisin',
 'feature_vedic_laryngitic_battuta',
 'feature_lotic_anticorrosive_acetone',
 'feature_hungry_thermoluminescent_shareholding',
 'feature_multiplicate_taunt_vena',
 'feature_rationalist_crippled_dreadnaught',
 'feature_blue_swedenborgianism_squinter',
 'feature_pictographic_epithetic_pleiocene',
 'feature_rival_leninist_philippine',
 'feature_unsafe_perspectivist_dairyman',
 'feature_verism_awing_countrywoman',
 'feature_grandmotherly_circumnavigable_homonymity',
 'feature_twinning_inconsequent_quercetin',
 'feature_dustiest_algonkian_plasticity',
 'feature_formational_preventive_capp',
 'feature_ruptured_activist_twill',
 'feature_inapproachable_aging_lully',
 'feature_tidied_tinnier_sacker',
 'feature_crookbacked_irreplaceable_heresiography',
 'feature_twentieth_pampered_statocyst',
 'feature_burled_zinky_verdin',
 'feature_unhygienic_brevipennate_viol',
 'feature_glued_antemundane_eschscholtzia',
 'feature_rockier_overdelicate_barostat',
 'feature_persnickety_schmalziest_dolichocephaly',
 'feature_phenomenal_photosynthetic_berg',
 'feature_peritectic_medallic_leaper',
 'feature_gripple_bracteal_huckster',
 'feature_holophytic_fiercest_ruining',
 'feature_octave_tingly_flitch',
 'feature_beauish_seismal_lobe',
 'feature_campestral_salted_romanism',
 'feature_eclectic_citified_testator',
 'feature_unministerial_albinotic_salish',
 'feature_tercentenary_triliteral_formosa',
 'feature_porky_gyratory_whitewall',
 'feature_dissilient_stunning_ribwort',
 'feature_ovidian_compelling_popsy',
 'feature_semestral_unapplausive_warfarin',
 'feature_newish_vortical_upthrow',
 'feature_compurgatorial_demeaning_purser',
 'feature_antiphrastical_tipsy_chinquapin',
 'feature_provable_downstair_titicaca',
 'feature_dorsolumbar_dolesome_marshmallow',
 'feature_trochaic_metonymic_dichogamy',
 'feature_antiscorbutic_visored_convalescent',
 'feature_unblamed_mammoth_commie',
 'feature_wiliest_specialistic_azrael',
 'feature_kenotic_hispanic_yulan',
 'feature_fine_aberrational_deducibility',
 'feature_reachable_angled_irrevocability',
 'feature_cachectical_metaphysic_patency',
 'feature_gory_ratite_santir',
 'feature_derogative_sawdusty_photolithography',
 'feature_conciliable_fleeting_equestrienne',
 'feature_newish_eosinophilic_sciaenoid',
 'feature_opposable_argumentative_mesoblast',
 'feature_guileful_congenerical_literation',
 'feature_liquefacient_premium_pyxidium',
 'feature_sleaziest_neediest_yawn',
 'feature_unforeboding_cometic_billionaire',
 'feature_jubilant_ghostlier_pedestrianism',
 'feature_doctrinal_popular_disqualifying',
 'feature_pseudo_ungeared_dullard',
 'feature_sclerotic_resupinate_hard',
 'feature_encased_jutting_gasometer',
 'feature_calycled_necrologic_naboth',
 'feature_teenage_tapetal_infector',
 'feature_undebauched_conservatory_doronicum',
 'feature_fescennine_intramundane_flamen',
 'feature_gymnastic_erective_tranche',
 'feature_phantasmal_stomachal_peperoni',
 'feature_pulverulent_tarnal_sitting',
 'feature_hetero_overweary_hylotheist',
 'feature_imponderable_seasoned_kinetograph',
 'feature_anoxic_froward_europe',
 'feature_antiperspirant_degradable_voltaire',
 'feature_stockier_ghastlier_spectrogram',
 'feature_appalachian_realistic_butyrate',
 'feature_mobile_savoyard_maidenhead',
 'feature_destructive_winterweight_towbar',
 'feature_jammed_morphotic_destruction',
 'feature_vaunting_offshore_bogey',
 'feature_floricultural_unimpeded_nomen',
 'feature_unwonted_trusted_fixative',
 'feature_affined_pushier_haranguer',
 'feature_unadjusted_citified_tupelo',
 'feature_down_delinquent_madagascar',
 'feature_nicer_fernier_banderilla',
 'feature_laggard_mincing_gemologist',
 'feature_cauterant_paly_discant',
 'feature_wastable_nobler_peeling',
 'feature_preparatory_sown_rehearser',
 'feature_articulated_shamefaced_orzo',
 'feature_scholiastic_dreamful_wardship',
 'feature_resoluble_shortened_albumin',
 'feature_crowning_frustrate_kampala',
 'feature_unforbidden_highbrow_kafir',
 'feature_equiprobable_unsisterly_mississauga',
 'feature_jumping_homemaking_shaveling',
 'feature_gleetier_unpared_pentecost',
 'feature_fishier_groggier_corporeity',
 'feature_lashed_dirigible_scrubber',
 'feature_unstaying_contrary_ichor',
 'feature_conventionalized_volitant_citadel',
 'feature_tepidity_endocrinal_altaic',
 'feature_servomechanical_wally_picul',
 'feature_resonant_elusive_monetisation',
 'feature_thalamencephalic_derisory_carte',
 'feature_ratable_wrapround_epitome',
 'feature_often_awry_chyme',
 'feature_limitative_traditional_yodeller',
 'feature_earthlier_parian_pistachio',
 'feature_clashing_disheartening_rattler',
 'feature_monocarpic_grey_hanger',
 'feature_macrocephalic_tiresome_piroshki',
 'feature_unshaped_compound_trunkfish',
 'feature_malarian_multinuclear_averroist',
 'feature_thousandfold_cairned_commutator',
 'feature_incog_decolorant_refluence',
 'feature_gay_deceased_reen',
 'feature_fortunate_unorthodoxy_trish',
 'feature_impartable_unpeaceful_pennsylvanian',
 'feature_maintained_childbearing_isagoge',
 'feature_illegible_punishing_lunacy',
 'feature_craziest_cordate_ravishment',
 'feature_voltairian_blackened_lodge',
 'feature_renowned_phthisic_ferrate',
 'feature_jeffersonian_torrent_cobaltite',
 'feature_reflected_vast_rhetor',
 'feature_arithmetic_delicate_pantagruelist',
 'feature_sooth_declinatory_chaton',
 'feature_qualifiable_dragging_mankind',
 "feature_whatsoe'er_discredited_faerie",
 'feature_prepotent_swedish_superexcellence',
 'feature_principled_inspiratory_loupe',
 'feature_aesthetical_daltonian_anemology',
 'feature_ectodermal_unfeigning_chester',
 'feature_continental_archival_katmandu',
 'feature_fraternal_whackiest_poppy',
 'feature_polymorphic_climactic_corslet',
 'feature_contraband_pyorrhoeal_vernalisation',
 'feature_sainted_nosiest_covenant',
 'feature_subcultural_overbearing_assiduity',
 'feature_airborne_genitival_cathouse',
 'feature_adventive_eristic_wedlock',
 'feature_triadic_vitreum_gelding',
 'feature_accusatory_disinfectant_deportment',
 'feature_gainly_tritheism_syndactyl',
 'feature_canalicular_peeling_lilienthal',
 'feature_enslaved_fattest_radiotelegraph',
 'feature_childing_funest_turbidity',
 'feature_pycnostyle_unenjoyable_balladry',
 'feature_sludgiest_telltale_mutant',
 'feature_tested_kinkiest_levigation',
 'feature_mailed_harried_grant',
 'feature_diabasic_prescribed_autograft',
 'feature_persuasible_ablest_menology',
 'feature_budgetary_surrounded_formicary',
 'feature_neurophysiological_runcinate_yardang',
 'feature_antichristian_slangiest_idyllist',
 'feature_flintier_enslaved_borsch',
 'feature_exorbitant_myeloid_crinkle',
 'feature_moralistic_heartier_typhoid',
 'feature_gutta_exploitive_simpson',
 'feature_assertive_worsened_scarper',
 'feature_intersubjective_juristic_sagebrush',
 'feature_rowable_unshod_noise',
 'feature_volitional_ascensive_selfhood',
 'feature_barest_kempt_crowd',
 'feature_curtained_gushier_tranquilizer',
 'feature_intermontane_vertical_moo',
 'feature_isotopic_hymenial_starwort',
 'feature_inseminated_filarial_mesoderm',
 'feature_passerine_ultraist_neon',
 'feature_westering_immunosuppressive_crapaud',
 'feature_reported_slimy_rhapsody',
 'feature_juvenalian_paunchy_uniformitarianism',
 'feature_lipogrammatic_blowsier_seismometry',
 'feature_draconic_contractible_romper',
 'feature_indirect_concrete_canaille',
 'feature_puberulent_nondescript_laparoscope',
 'feature_unsurveyed_boyish_aleph',
 'feature_dismaying_chaldean_tallith',
 'feature_epitaxial_loathsome_essen',
 'feature_malagasy_abounding_circumciser',
 'feature_tortured_arsenical_arable',
 'feature_sludgy_implemental_sicily',
 'feature_uretic_seral_decoding',
 'feature_roasting_slaked_reposition',
 'feature_peculiar_sheenier_quintal',
 'feature_publishable_apiarian_rollick',
 'feature_interdental_mongolian_anarchism',
 'feature_reserved_cleanable_soldan',
 'feature_eruptive_seasoned_pharmacognosy',
 'feature_softish_unseparated_caudex',
 'feature_univalve_abdicant_distrail',
 'feature_levigate_kindly_dyspareunia',
 'feature_intercalative_helvetian_infirmarian',
 'feature_centric_shaggier_cranko',
 'feature_unliving_bit_bengaline',
 'feature_misanthropic_knurliest_freebooty',
 'feature_confiscatory_triennial_pelting',
 'feature_peltate_okay_info',
 'feature_reconciling_dauby_database',
 'feature_unspotted_practiced_gland',
 'feature_iconomatic_boozier_age',
 'feature_congenial_transmigrant_isobel',
 'feature_impractical_endorsed_tide',
 'feature_huskiest_compartmental_jacquerie',
 'feature_cairned_fumiest_ordaining',
 'feature_calculating_unenchanted_microscopium',
 'feature_mucky_loanable_gastrostomy',
 'feature_loricate_cryptocrystalline_ethnology',
 'feature_ctenoid_moaning_fontainebleau',
 'feature_armoured_finable_skywriter',
 'feature_intended_involute_highbinder',
 'feature_commensurable_industrial_jungfrau',
 'feature_spagyric_echt_alum',
 'feature_illiterate_stomachal_terpene',
 'feature_demure_groutiest_housedog',
 'feature_dentilingual_removed_osmometer',
 'feature_plexiform_won_elk',
 'feature_induplicate_hoarse_disbursement',
 'feature_vizierial_courtlier_hampton',
 'feature_lost_quirky_botel',
 'feature_hydrologic_cymric_nyctophobia',
 'feature_atlantic_uveal_incommunicability',
 'feature_incommensurable_diffused_curability',
 'feature_midget_noncognizable_plenary',
 'feature_unapplicable_jerkiest_klemperer',
 'feature_unamazed_tumular_photomicrograph',
 'feature_nucleophilic_uremic_endogen',
 'feature_unnourishing_indiscreet_occiput',
 'feature_hypersonic_volcanological_footwear',
 'feature_uncomplimentary_malignant_scoff',
 'feature_interrogatory_isohyetal_atacamite',
 'feature_invalid_chromatographic_cornishman',
 'feature_encompassing_skeptical_salience',
 'feature_beady_unkind_barret',
 'feature_gossamer_placable_wycliffite',
 'feature_flavourful_seismic_erica',
 'feature_petitionary_evanescent_diallage',
 'feature_leisurable_dehortatory_pretoria',
 'feature_chaotic_granitoid_theist',
 'feature_cerebrovascular_weeny_advocate',
 'feature_cyrenaic_unschooled_silurian',
 'feature_uninclosed_handcrafted_springing',
 'feature_indentured_communicant_tulipomania',
 'feature_synoptic_botryose_earthwork',
 'feature_acerb_venusian_piety',
 'feature_vestmental_hoofed_transpose',
 'feature_unrated_intact_balmoral',
 'feature_tonal_graptolitic_corsac',
 'feature_refreshed_untombed_skinhead',
 'feature_communicatory_unrecommended_velure',
 'feature_sudsy_polymeric_posteriority',
 'feature_inexpugnable_gleg_candelilla',
 'feature_headhunting_unsatisfied_phenomena',
 'feature_scenographical_dissentient_trek',
 'feature_trim_axial_suffocation',
 'feature_buxom_curtained_sienna',
 'feature_recidivism_petitory_methyltestosterone',
 'feature_irresponsive_compositive_ramson',
 'feature_more_hindoo_diageotropism',
 'feature_apomictical_motorized_vaporisation',
 'feature_seclusive_emendatory_plangency',
 'feature_discrepant_ventral_shicker',
 'feature_obeisant_vicarial_passibility',
 'feature_unrelieved_rawish_cement',
 'feature_strychnic_structuralist_chital',
 'feature_unaimed_yonder_filmland',
 'feature_designer_notchy_epiploon',
 'feature_favoring_prescript_unorthodoxy',
 'feature_cheering_protonemal_herd',
 'feature_conjugal_postvocalic_rowe',
 'feature_palmy_superfluid_argyrodite',
 'feature_outsized_admonishing_errantry',
 'feature_brawny_confocal_frail',
 'feature_arillate_nickelic_hemorrhage',
 'feature_bloodied_twinkling_andante',
 'feature_rusted_unassisting_menaquinone',
 'feature_biannual_maleficent_thack',
 'feature_extractable_serrulate_swing',
 'feature_covalent_methodological_brash',
 'feature_periscopic_thirteenth_cartage',
 'feature_tranquilizing_abashed_glyceria',
 'feature_faustian_unventilated_lackluster',
 'feature_frequentative_participial_waft',
 'feature_flakiest_fleecy_novelese',
 'feature_liege_unexercised_ennoblement',
 'feature_amygdaloidal_intersectional_canonry',
 'feature_rural_inquisitional_trotline',
 'feature_caespitose_unverifiable_intent',
 'feature_precooled_inoperable_credence',
 'feature_ruthenian_uncluttered_vocalizing',
 'feature_camphorated_spry_freemartin',
 'feature_mined_game_curse',
 'feature_amoebaean_wolfish_heeler',
 'feature_peaty_vulgar_branchia',
 'feature_ratlike_matrilinear_collapsability',
 'feature_attuned_southward_heckle',
 'feature_substandard_permissible_paresthesia',
 'feature_curling_aurorean_iseult',
 'feature_fleshly_bedimmed_enfacement',
 'feature_hendecagonal_deathly_stiver',
 'feature_massed_nonracial_ecclesiologist',
 'feature_instrumentalist_extrovert_cassini',
 'feature_patristical_analysable_langouste',
 'feature_gullable_sanguine_incongruity',
 'feature_phellogenetic_vibrational_jocelyn',
 'feature_autodidactic_gnarlier_pericardium',
 'feature_unbeaten_orological_dentin',
 'feature_resuscitative_communicable_brede',
 'feature_limitable_astable_physiology',
 'feature_chartered_conceptual_spitting',
 'feature_oversea_permed_insulter',
 'feature_busty_unfitted_keratotomy',
 'feature_alkaline_pistachio_sunstone',
 'feature_incitant_trochoidal_oculist',
 'feature_stelar_balmiest_pellitory',
 'feature_hypermetropic_unsighted_forsyth',
 'feature_calycled_living_birmingham',
 'feature_transmontane_clerkly_value',
 'feature_together_suppositive_aster',
 'feature_planned_superimposed_bend',
 'feature_undirected_perdu_ylem',
 'feature_antipodal_unable_thievery',
 'feature_scrobiculate_unexcitable_alder',
 'feature_apophthegmatical_catechetical_millet',
 'feature_fragrant_fifteen_brian',
 'feature_churrigueresque_talc_archaicism',
 'feature_uncompromising_fancy_kyle',
 'feature_retinoscopy_flinty_wool',
 'feature_pansophic_merino_pintado',
 'feature_galvanometric_sturdied_billingsgate',
 'feature_mazy_superrefined_punishment',
 'feature_aztecan_encomiastic_pitcherful',
 'feature_sorted_ignitable_sagitta',
 'feature_padded_peripteral_pericranium',
 'feature_trabeate_eutherian_valedictory',
 'feature_undisguised_whatever_gaul',
 'feature_migrant_reliable_chirurgery',
 'feature_permanent_cottony_ballpen',
 'feature_conceding_ingrate_tablespoonful',
 'feature_vulcanological_sepulchral_spean',
 'feature_unrequired_waxing_skeptic',
 'feature_hibernating_soritic_croupe',
 'feature_unstacked_trackable_blizzard',
 'feature_multilinear_sharpened_mouse',
 'feature_autarkic_constabulary_dukedom',
 'feature_christadelphian_euclidean_boon',
 'feature_covalent_unreformed_frogbit',
 'feature_basaltic_arid_scallion',
 'feature_uncharged_unovercome_smolder',
 'feature_leaky_maroon_pyrometry',
 'feature_polaroid_squalliest_applause',
 'feature_jerkwater_eustatic_electrocardiograph',
 'feature_malacological_differential_defeated',
 'feature_ambisexual_boiled_blunderer',
 'feature_endangered_unthreaded_firebrick',
 'feature_unextinct_smectic_isa',
 'feature_kerygmatic_splashed_ziegfeld',
 'feature_palatalized_unsucceeded_induration',
 'feature_springlike_crackjaw_bheesty',
 'feature_unweary_congolese_captain',
 'feature_uncertified_myrmecological_nagger',
 'feature_hemispherical_unabsolved_aeolipile',
 'feature_glyptic_unrubbed_holloway',
 'feature_rimmed_conditional_archipelago',
 'feature_bleeding_arabesque_pneuma',
 'feature_dipped_sent_giuseppe',
 'feature_undivorced_unsatisfying_praetorium',
 'feature_reclinate_cruciform_lilo',
 'feature_dermal_extortive_inversion',
 'feature_tiniest_rangiest_investment',
 'feature_creedal_greediest_vibrant',
 'feature_plausive_carboxyl_elaborator',
 'feature_undemocratic_unrhymed_subarea',
 'feature_theocratical_shredded_categorist',
 'feature_hedonistic_entertaining_pelting',
 'feature_placable_conscionable_mickey',
 'feature_crawlier_residuary_biracialism',
 'feature_close_abloom_boomlet',
 'feature_virtual_standard_instigator',
 'feature_hagiographical_swelling_homager',
 'feature_stereo_orthotone_infernal',
 'feature_brittle_chautauqua_elite',
 'feature_antinomical_swart_coo',
 'feature_thumblike_aniconic_plantagenet',
 'feature_stentorian_contraband_swarajism',
 'feature_coveted_finniest_lipogram',
 'feature_cut_rouged_defilement',
 'feature_nude_revived_unpredictability',
 'feature_subject_shaggier_relapse',
 'feature_fraternal_manx_tambourine',
 'feature_clarified_mimic_slash',
 'feature_invincible_unroofed_tetrachord',
 'feature_phantasmagorial_lyophilized_chippendale',
 'feature_deathy_hylomorphic_christine',
 'feature_antediluvial_contributive_statecraft',
 'feature_equalized_statuary_more',
 'feature_difficile_oxalic_spheroid',
 'feature_macrobiotic_benignant_eurasia',
 'feature_secernent_condensed_lynchet',
 'feature_ternary_plump_floridity',
 'feature_sumatran_lobose_novice',
 'feature_twilled_eliminable_orthoepy',
 'feature_photolithographic_attenuate_serapeum',
 'feature_umbonate_haired_grisette',
 'feature_gobony_configured_judicatory',
 'feature_ruthenic_pinchbeck_limitarian',
 'feature_acoustical_infirm_ascent',
 'feature_saclike_pulverizable_doornail',
 'feature_shielded_specialized_goalie',
 'feature_institutionalized_individualized_worcestershire',
 'feature_unmentioned_peddling_arboretum',
 'feature_gabby_glimmery_azalea',
 'feature_saturate_unsentimental_allograph',
 'feature_interdigital_sewn_glucagon',
 'feature_watery_behaviorist_incitation',
 'feature_unshuttered_curly_uvular',
 'feature_bareheaded_satisfied_copartner',
 'feature_unseasonable_weathered_med',
 'feature_undersexed_agone_metaplasm',
 'feature_septicemic_telangiectatic_flake',
 'feature_correlative_nociceptive_development',
 'feature_literate_treacly_altercation',
 'feature_nipping_pinnate_attempter',
 'feature_eradicable_douce_teleconference',
 'feature_prettyish_lapsed_manifest',
 'feature_hierologic_assault_candida',
 'feature_pellicular_comeliest_charlatan',
 'feature_cottony_ischaemic_venery',
 'feature_undersealed_waxiest_floriculturist',
 'feature_upturned_epicanthic_galilee',
 'feature_tweedy_noncontroversial_poisoner',
 'feature_extrorse_idling_vinificator',
 'feature_wavier_estranged_handstand',
 'feature_interjaculatory_anhedonic_lindisfarne',
 'feature_unmechanized_completed_specializer',
 'feature_residentiary_rollable_boaz',
 'feature_hypertensive_aroused_urquhart',
 'feature_seven_entomological_boothose',
 'feature_lageniform_theodicean_terylene',
 'feature_unattached_flowing_longfellow',
 'feature_hypertensive_seared_adenauer',
 'feature_cambial_bigoted_bacterioid',
 'feature_attributable_shagged_nourishment',
 'feature_randomized_dystonic_knar',
 'feature_pedal_dissident_thummim',
 'feature_submediant_serbonian_rangefinder',
 'feature_philologic_errhine_nubbin',
 'feature_unlike_guelfic_sculp',
 'feature_theophanic_winded_passacaglia',
 'feature_haploid_occluded_moral',
 'feature_livelong_sounded_squirm',
 'feature_mandibulate_whining_localizer',
 'feature_heliotropic_inane_papistry',
 'feature_treed_nomographic_trivet',
 'feature_sulkier_inebriated_sadduceeism',
 'feature_distorted_bit_frilling',
 'feature_subovate_deicidal_brett',
 'feature_devoted_straggly_charlotte',
 'feature_unlike_creeping_mammock',
 'feature_antarthritic_rhinocerotic_marquette',
 'feature_resupine_pectic_marathonian',
 'feature_madrigalian_distichal_otranto',
 'feature_systematic_nappiest_bruiser',
 'feature_pinioned_desiccated_renegade',
 'feature_aliped_subclavicular_nasopharynx',
 'feature_branched_dilatory_sunbelt',
 'feature_biochemical_searching_sodalite',
 'feature_unsealed_suffixal_babar',
 'feature_forficate_martensitic_huckaback',
 'feature_silver_handworked_scauper',
 'feature_interpenetrative_enabling_elm',
 'feature_wholesale_contrived_caracole',
 'feature_wry_bursal_treadler',
 'feature_obverse_plumy_ultramontanism',
 'feature_zoochemical_brimful_hector',
 'feature_mouldiest_clostridial_paper',
 'feature_unabolished_monographical_egestion',
 'feature_hawaiian_adrenocorticotropic_right',
 'feature_preservable_panchromatic_roadhouse',
 'feature_pozzolanic_keltic_nymphomania',
 'feature_spiroid_chuffy_dachshund',
 'feature_prefab_stabbing_declivity',
 'feature_lethal_stringy_substantial',
 'feature_lowermost_tetrastichic_transvestism',
 'feature_huge_neoclassic_crossbencher',
 'feature_unsensualized_classier_professionalisation',
 'feature_weldable_beribboned_margate',
 'feature_alleviatory_novercal_semivowel',
 'feature_dwarfish_equestrian_vaporizer',
 'feature_limnological_proboscidean_nomologist',
 'feature_unlocked_tetraethyl_mobility',
 'feature_contralto_unblushing_beowulf',
 'feature_shouted_weary_hagberry',
 'feature_unwarrantable_egotistic_guayule',
 'feature_canonical_dermatographic_mellowing',
 'feature_unaired_operose_lactoprotein',
 'feature_halting_disguisable_syllabicity',
 'feature_aristotelian_eurasian_nooky',
 'feature_titillative_pledged_breccia',
 'feature_parallelism_later_triennial',
 'feature_cupulate_mettlesome_extensimeter',
 'feature_lowliest_oblatory_sprattle',
 'feature_fluorometric_guarded_dioxin',
 'feature_enorm_taboo_abysm',
 'feature_compliable_buccaneerish_lyddite',
 'feature_sulpha_wiring_ambiance',
 'feature_faeroese_inapposite_plum',
 'feature_fake_lentoid_helotism',
 'feature_gardant_irreverent_inferiority',
 'feature_munificent_stupid_hitlerism',
 'feature_driving_neurasthenic_perimysium',
 'feature_arborescent_fungal_despiser',
 'feature_unbenign_unpromising_aglet',
 'feature_conscionable_insouciant_pariah',
 'feature_inconsequential_scincoid_sextant',
 'feature_boyish_ductile_ninety',
 'feature_spongiest_sunbeamed_connolly',
 'feature_unbagged_parliamentary_plough',
 'feature_unshingled_dotier_bose',
 'feature_unfathered_roofed_taskmaster',
 'feature_unwatered_sightable_hardtack',
 'feature_teriyaki_splashy_inaptitude',
 'feature_frumpy_squishiest_burley',
 'feature_archaean_seething_hoyle',
 'feature_subsidiary_enzymatic_hedonist',
 'feature_oblate_quivery_bottrop',
 'feature_multicostate_undescended_monoacid',
 'feature_gradely_dippy_gaol',
 ...]
# train models on subsamples eras in df1
lgb1 = lightgbm.LGBMRegressor()
lgb1.fit(df1[eras.isin(np.arange(1, 304, 4))][features], df1[eras.isin(np.arange(1, 304, 4))][target])

lgb2 = lightgbm.LGBMRegressor()
lgb2.fit(df1[eras.isin(np.arange(2, 304, 4))][features], df1[eras.isin(np.arange(2, 304, 4))][target])

lgb3 = lightgbm.LGBMRegressor()
lgb3.fit(df1[eras.isin(np.arange(3, 304, 4))][features], df1[eras.isin(np.arange(3, 304, 4))][target])

lgb4 = lightgbm.LGBMRegressor()
lgb4.fit(df1[eras.isin(np.arange(4, 304, 4))][features], df1[eras.isin(np.arange(4, 304, 4))][target])
<ipython-input-40-5fc463007f8b>:3: UserWarning: Boolean Series key will be reindexed to match DataFrame index.
  lgb1.fit(df1[eras.isin(np.arange(1, 304, 4))][features], df1[eras.isin(np.arange(1, 304, 4))][target])
<ipython-input-40-5fc463007f8b>:6: UserWarning: Boolean Series key will be reindexed to match DataFrame index.
  lgb2.fit(df1[eras.isin(np.arange(2, 304, 4))][features], df1[eras.isin(np.arange(2, 304, 4))][target])
<ipython-input-40-5fc463007f8b>:9: UserWarning: Boolean Series key will be reindexed to match DataFrame index.
  lgb3.fit(df1[eras.isin(np.arange(3, 304, 4))][features], df1[eras.isin(np.arange(3, 304, 4))][target])
<ipython-input-40-5fc463007f8b>:12: UserWarning: Boolean Series key will be reindexed to match DataFrame index.
  lgb4.fit(df1[eras.isin(np.arange(4, 304, 4))][features], df1[eras.isin(np.arange(4, 304, 4))][target])
LGBMRegressor()
# calculate predictions for each model
preds = [
        pd.Series(model.predict(df2[features]), index=df2.index)
    for model in [lgb_all, lgb1, lgb2, lgb3, lgb4]
]
# numerai score of model trained on all eras of df1
numerai_score(df2[target], preds[0])
0.047100803265479066
# numerai score of ensemble of models trained on all subsampled eras of df1
numerai_score(df2[target], pd.concat(preds, axis=1)[[1,2,3,4]].mean(axis=1))
0.05025499943392698
Gotcha: eras are homogenous, but different from each other
Random cross-validation will look much better than cross-validating by era
Even for a simple linear model, taking a random shuffle reports a correlation of ~5.1%, but a time series split reports a lower score of ~2.6%

Gotcha: Eras are now overlapping!
Even era-wise cross-validation will now have leakage!
Eras are weekly (5 days), but the target is four weeks out (20 days). You can either use blocks of eras and purge the overlapping sections or subsample the eras (every 4th era) to make them not overlapping and prevent leakage

# Subsample data to every 4th era to prevent overlapping eras
df = df[eras.isin(np.arange(1, 575, 4))]
df1 = df[eras<=eras.median()]
df2 = df[eras>eras.median()]
<ipython-input-44-73ffccb7a00f>:3: UserWarning: Boolean Series key will be reindexed to match DataFrame index.
  df1 = df[eras<=eras.median()]
<ipython-input-44-73ffccb7a00f>:4: UserWarning: Boolean Series key will be reindexed to match DataFrame index.
  df2 = df[eras>eras.median()]
eras = eras[eras.isin(np.arange(1, 575, 4))]
# Because the TimeSeriesSplit class in sklearn does not use groups and won't respect era boundries, we implement
# a version that will

from sklearn.model_selection._split import _BaseKFold, indexable, _num_samples

class TimeSeriesSplitGroups(_BaseKFold):
    def __init__(self, n_splits=5):
        super().__init__(n_splits, shuffle=False, random_state=None)

    def split(self, X, y=None, groups=None):
        X, y, groups = indexable(X, y, groups)
        n_samples = _num_samples(X)
        n_splits = self.n_splits
        n_folds = n_splits + 1
        group_list = np.unique(groups)
        n_groups = len(group_list)
        if n_folds > n_groups:
            raise ValueError(
                ("Cannot have number of folds ={0} greater"
                 " than the number of samples: {1}.").format(n_folds,
                                                             n_groups))
        indices = np.arange(n_samples)
        test_size = (n_groups // n_folds)
        test_starts = range(test_size + n_groups % n_folds,
                            n_groups, test_size)
        test_starts = list(test_starts)[::-1]
        for test_start in test_starts:
            
            yield (indices[groups.isin(group_list[:test_start])],
                   indices[groups.isin(group_list[test_start:test_start + test_size])])
crossvalidators = [
    model_selection.KFold(5),
    model_selection.KFold(5, shuffle=True),
    model_selection.GroupKFold(5),
    TimeSeriesSplitGroups(5)
]
def correlation_score(y_true, y_pred):
    return np.corrcoef(y_true, y_pred)[0,1]
for cv in crossvalidators:
    print(cv)
    print(np.mean(
            model_selection.cross_val_score(
            linear_model.LinearRegression(),
            df[features],
            df[target],
            cv=cv,
            n_jobs=1,
            groups=eras,
            scoring=metrics.make_scorer(correlation_score, greater_is_better=True)
        )))
    print()
KFold(n_splits=5, random_state=None, shuffle=False)
0.04081357263615691

KFold(n_splits=5, random_state=None, shuffle=True)
0.05205511367759082

GroupKFold(n_splits=5)
0.04341132164952086

TimeSeriesSplitGroups(n_splits=5)
0.02598605396562758


Gotcha: {0, 1} are noticeably different from {0.25, 0.75}
This makes training a classifier one-versus-rest behave counterintuitively.

Specifically, a multinomial classifier seem to learn how to pick out extreme targets, and their predictions are the most correlated

# Train a standard logistic regression as a classifier
logistic = linear_model.LogisticRegression(multi_class='multinomial', solver='saga')
logistic.fit(df1[features], (df1[target]*4).astype(int))
logistic.score(df1[features], (df1[target]*4).astype(int))
0.4997459543647697
# The first and last class are highly correlated
corrs=np.corrcoef(logistic.predict_proba(df2[features]).T)
plt.imshow(corrs, vmin=-1, vmax=1, cmap="RdYlGn")
corrs
array([[ 1.        ,  0.34062985, -0.83573069,  0.28275667,  0.75927678],
       [ 0.34062985,  1.        , -0.58813758,  0.17450404,  0.22026688],
       [-0.83573069, -0.58813758,  1.        , -0.62462003, -0.8344854 ],
       [ 0.28275667,  0.17450404, -0.62462003,  1.        ,  0.38212384],
       [ 0.75927678,  0.22026688, -0.8344854 ,  0.38212384,  1.        ]])

# Out-of-sample correlation is 3.5%
preds = pd.Series(logistic.predict_proba(df2[features]).dot(logistic.classes_), index=df2.index)
numerai_score(df2[target], preds)
0.03504082804703481
# A standard linear model has a slightly lower correlation out-of-sample
linear = linear_model.LinearRegression()
linear.fit(df1[features], df1[target])
linear.score(df2[features], df2[target])
preds = pd.Series(linear.predict(df2[features]), index=df2.index)
numerai_score(df2[target], preds)
0.03469854472021291
Eras can be more or less applicable to other eras
You can test this be splitting the eras into blocks of 15, training on each block, and evaluating on each other block.

# convert the subsampled era labels to continuous integers
eras_sub = (eras-1)//4
eras15 = (eras_sub // 15) * 15
eras15.value_counts()
60     71000
105    69251
120    67610
45     67268
90     66300
75     61933
30     61333
15     54214
0      42920
135    42282
Name: erano, dtype: int64
results15 = []
for train_era, tdf in df.groupby(eras15):
    print(train_era)
    model = linear_model.LinearRegression()
    model.fit(tdf[features], tdf[target])
    for test_era, tdf in df.groupby(eras15):
        results15.append([
            train_era,
            test_era,
            correlation_score(tdf[target], model.predict(tdf[features]))
        ])
0
15
30
45
60
75
90
105
120
135

results_df = pd.DataFrame(
    results15,
    columns=["train_era", "test_era", "score"]
).pivot(index="train_era", columns="test_era", values="score")
results_df
test_era	0	15	30	45	60	75	90	105	120	135
train_era										
0	0.194921	0.043216	0.030027	0.016123	0.009126	0.023706	0.033789	0.022836	0.024734	0.020147
15	0.046587	0.170537	0.029796	-0.007569	0.002888	0.001372	0.000648	0.001539	-0.007086	-0.005150
30	0.039467	0.032860	0.156099	0.002623	0.003846	0.006211	0.005686	-0.001824	-0.000756	-0.002574
45	0.023706	0.030027	0.035148	0.148862	0.010267	0.021071	0.027580	0.015904	0.031235	0.023599
60	0.012483	0.006499	0.009025	0.016631	0.149721	0.019196	0.010640	0.013178	0.010896	0.011191
75	0.025632	0.018699	0.031269	0.016043	0.018465	0.166712	0.026355	0.017058	0.021476	0.011319
90	0.044772	0.025483	0.024666	0.024288	0.005159	0.029021	0.153414	0.024905	0.021045	0.016773
105	0.029213	0.027908	0.018279	0.012079	0.008206	0.016061	0.027251	0.143886	0.023365	0.009489
120	0.033693	0.036589	0.018515	0.030186	0.007597	0.026739	0.023073	0.028707	0.144769	0.030703
135	0.024917	0.012725	0.011641	0.019961	0.004650	0.011544	0.013249	0.010797	0.025249	0.184200
# Each row here is the training block of eras, each column is a testing block of eras.
# Note that there is a period in the early groups that does not seem to be relevant to other eras, and the
# overall performance seems to decrease a bit over time.
plt.figure(figsize=(15,15))
plt.imshow(results_df, vmin=-0.04, vmax=0.04, cmap="RdYlGn")
<matplotlib.image.AxesImage at 0x7f2ab6141a60>

Here is an advanced paper that talks about generalization. Eras can be thought about in the same way that "distributions" or "environments" are talked about here https://arxiv.org/pdf/1907.02893.pdf

Gotcha: Since the signal-to-noise ratio is so low, models can take many more iterations than expected, and have scarily high in-sample performance
def our_score(preds, dtrain):
    return "score", -np.corrcoef(preds, dtrain.get_label())[0,1], False

df1 = df[eras<=eras.median()]
df2 = df[eras>eras.median()]
dtrain = lightgbm.Dataset(df1[features], df1[target])
dtest = lightgbm.Dataset(df2[features], df2[target])
dall = lightgbm.Dataset(df[features], df[target])
param = {
    'max_depth':3,
    'eta':0.1,
    'objective':'regression',
    'metric':'mse',
    'nthread': -1,
}
evals_result = {}
bst = lightgbm.train(
    params=param,
    train_set=dtrain,
    feval=our_score,
    num_boost_round=500,
    valid_sets=[dtrain, dtest],
    valid_names=['train', 'test'],
    evals_result=evals_result,
    verbose_eval=20,
)

(-pd.DataFrame({k: v['score'] for k,v in evals_result.items()})).plot(ylim=[0,0.25])
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.395800 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 5250
[LightGBM] [Info] Number of data points in the train set: 305247, number of used features: 1050
[LightGBM] [Info] Start training from score 0.499998
[20]	train's l2: 0.0498386	train's score: -0.0696572	test's l2: 0.0499211	test's score: -0.042272
[40]	train's l2: 0.0497466	train's score: -0.0845498	test's l2: 0.0499021	test's score: -0.0449487
[60]	train's l2: 0.0496736	train's score: -0.0950992	test's l2: 0.049894	test's score: -0.0462429
[80]	train's l2: 0.0496094	train's score: -0.103748	test's l2: 0.0498883	test's score: -0.0474028
[100]	train's l2: 0.0495503	train's score: -0.111415	test's l2: 0.049883	test's score: -0.0485958
[120]	train's l2: 0.0494963	train's score: -0.118318	test's l2: 0.0498832	test's score: -0.0487773
[140]	train's l2: 0.0494427	train's score: -0.124419	test's l2: 0.0498826	test's score: -0.0492182
[160]	train's l2: 0.0493929	train's score: -0.12975	test's l2: 0.0498849	test's score: -0.0492456
[180]	train's l2: 0.0493438	train's score: -0.135098	test's l2: 0.0498857	test's score: -0.0495008
[200]	train's l2: 0.0492992	train's score: -0.139802	test's l2: 0.0498864	test's score: -0.0497836
[220]	train's l2: 0.0492556	train's score: -0.144227	test's l2: 0.049888	test's score: -0.0499265
[240]	train's l2: 0.0492132	train's score: -0.148465	test's l2: 0.0498893	test's score: -0.0501424
[260]	train's l2: 0.0491713	train's score: -0.152511	test's l2: 0.0498917	test's score: -0.0502095
[280]	train's l2: 0.0491288	train's score: -0.156449	test's l2: 0.0498947	test's score: -0.0502408
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[300]	train's l2: 0.0490897	train's score: -0.160258	test's l2: 0.0498965	test's score: -0.0503664
[320]	train's l2: 0.0490502	train's score: -0.163728	test's l2: 0.0498975	test's score: -0.0507012
[340]	train's l2: 0.0490115	train's score: -0.167201	test's l2: 0.0499019	test's score: -0.0505273
[360]	train's l2: 0.0489723	train's score: -0.170573	test's l2: 0.0499006	test's score: -0.051169
[380]	train's l2: 0.0489356	train's score: -0.173804	test's l2: 0.0499038	test's score: -0.0510981
[400]	train's l2: 0.0488994	train's score: -0.177039	test's l2: 0.0499082	test's score: -0.0509207
[420]	train's l2: 0.0488637	train's score: -0.180099	test's l2: 0.0499135	test's score: -0.0505648
[440]	train's l2: 0.0488284	train's score: -0.182793	test's l2: 0.0499169	test's score: -0.0505659
[460]	train's l2: 0.0487914	train's score: -0.186067	test's l2: 0.0499181	test's score: -0.0508391
[480]	train's l2: 0.0487565	train's score: -0.18906	test's l2: 0.0499207	test's score: -0.0508337
[500]	train's l2: 0.048721	train's score: -0.192284	test's l2: 0.0499246	test's score: -0.0506581
<AxesSubplot:>

The results are sensitive to the choice of parameters, which should be picked through cross-validation
df1 = df[eras<=eras.median()]
df2 = df[eras>eras.median()]
models = [
    linear_model.LinearRegression(),
] + [
    linear_model.ElasticNet(alpha=alpha)
    for alpha in [0.01, 0.005, 0.002, 0.001, 0.0005, 0.0002, 0.0001, 0.00005, 0.00002, 0.00001]
] + [
    lightgbm.LGBMRegressor(n_jobs=-1),
    lightgbm.LGBMRegressor(n_jobs=-1, learning_rate=0.01, n_estimators=1000),
    lightgbm.LGBMRegressor(n_jobs=-1, colsample_bytree=0.1, learning_rate=0.01, n_estimators=1000),
    lightgbm.LGBMRegressor(n_jobs=-1, colsample_bytree=0.1, learning_rate=0.01, n_estimators=1000, max_depth=5),
    lightgbm.LGBMRegressor(n_jobs=-1, colsample_bytree=0.1, learning_rate=0.001, n_estimators=10000, max_depth=5),
]
for model in models:
    print(" -- ", model)   
    model.fit(df1[features], df1[target])
    outsample = numerai_score(df2[target], pd.Series(model.predict(df2[features]), index=df2.index))
    insample = numerai_score(df1[target], pd.Series(model.predict(df1[features]), index=df1.index))
    print(
        f"outsample: {outsample}, insample: {insample}"
    )
    print()
 --  LinearRegression()
outsample: 0.03449532642406725, insample: 0.08811577962731876

 --  ElasticNet(alpha=0.01)
outsample: -0.0005817296572705758, insample: 0.0011047675627246087

 --  ElasticNet(alpha=0.005)
outsample: -0.0005817296572705758, insample: 0.0011047675627246087

 --  ElasticNet(alpha=0.002)
outsample: 0.028356554996391864, insample: 0.036860395887039446

 --  ElasticNet(alpha=0.001)
outsample: 0.03991189570044001, insample: 0.05365003421490132

 --  ElasticNet(alpha=0.0005)
outsample: 0.04275518297522411, insample: 0.06223910243312713

 --  ElasticNet(alpha=0.0002)
outsample: 0.04408886141392253, insample: 0.07101617648855875

 --  ElasticNet(alpha=0.0001)
outsample: 0.042497050882822206, insample: 0.07683662591705001

 --  ElasticNet(alpha=5e-05)
outsample: 0.04031370910547076, insample: 0.0826422949678376

 --  ElasticNet(alpha=2e-05)
/home/michael/anaconda3/envs/xgb/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:530: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 187.98899841308594, tolerance: 1.5261499881744385
  model = cd_fast.enet_coordinate_descent(
outsample: 0.03723017817413682, insample: 0.08677474150444352

 --  ElasticNet(alpha=1e-05)
/home/michael/anaconda3/envs/xgb/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:530: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 759.37939453125, tolerance: 1.5261499881744385
  model = cd_fast.enet_coordinate_descent(
outsample: 0.03591553345108072, insample: 0.08774837309606072

 --  LGBMRegressor()
outsample: 0.04163519344042179, insample: 0.2413164724798182

 --  LGBMRegressor(learning_rate=0.01, n_estimators=1000)
outsample: 0.04716206072784002, insample: 0.27980942641432893

 --  LGBMRegressor(colsample_bytree=0.1, learning_rate=0.01, n_estimators=1000)
outsample: 0.049906177599943716, insample: 0.2486898095456977

 --  LGBMRegressor(colsample_bytree=0.1, learning_rate=0.01, max_depth=5,
              n_estimators=1000)
outsample: 0.051584782947410084, insample: 0.20542516882601236

 --  LGBMRegressor(colsample_bytree=0.1, learning_rate=0.001, max_depth=5,
              n_estimators=10000)
outsample: 0.05228707325079221, insample: 0.2077346244025685


Gotcha: Models with large exposures to individual features tend to perform poorly or inconsistently out of sample
import numpy as np
import scipy
# Train a standard xgboost on half the train eras
lgb = lightgbm.LGBMRegressor(n_estimators=1000, max_depth=5, learning_rate=0.01, n_jobs=-1)
lgb.fit(df1[features], df1[target])
lgb_preds = lgb.predict(df2[features])
Our predictions have correlation < -0.4 and > 0.20 for some single features!
Sure hope those features continue to act as they have in the past!

corr_list = []
for feature in features:
    corr_list.append(np.corrcoef(df2[feature], lgb_preds)[0,1])
corr_series = pd.Series(corr_list, index=features)
corr_series.describe()
count    1050.000000
mean        0.007197
std         0.084903
min        -0.431369
25%        -0.048999
50%         0.008305
75%         0.065700
max         0.195975
dtype: float64
from sklearn.preprocessing import MinMaxScaler
import scipy

def neutralize(df,
               columns,
               neutralizers=None,
               proportion=1.0,
               normalize=True,
               era_col="era"):
    if neutralizers is None:
        neutralizers = []
    unique_eras = df[era_col].unique()
    computed = []
    for u in unique_eras:
        print(u, end="\r")
        df_era = df[df[era_col] == u]
        scores = df_era[columns].values
        if normalize:
            scores2 = []
            for x in scores.T:
                x = (scipy.stats.rankdata(x, method='ordinal') - .5) / len(x)
                x = scipy.stats.norm.ppf(x)
                scores2.append(x)
            scores = np.array(scores2).T
        exposures = df_era[neutralizers].values

        scores -= proportion * exposures.dot(
            np.linalg.pinv(exposures.astype(np.float32)).dot(scores.astype(np.float32)))

        scores /= scores.std(ddof=0)

        computed.append(scores)

    return pd.DataFrame(np.concatenate(computed),
                        columns=columns,
                        index=df.index)
df2["preds"] = lgb_preds
df2["preds_neutralized"] = neutralize(df2, 
                                      columns=["preds"], 
                                      neutralizers=features, 
                                      proportion=0.5, # neutralize by 50% within each era
                                      normalize=True,
                                      era_col='era') 

scaler = MinMaxScaler()
df2["preds_neutralized"] = scaler.fit_transform(df2[["preds_neutralized"]]) # transform back to 0-1
<ipython-input-69-4aa87b479082>:1: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  df2["preds"] = lgb_preds
0573
<ipython-input-69-4aa87b479082>:2: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  df2["preds_neutralized"] = neutralize(df2,
<ipython-input-69-4aa87b479082>:10: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  df2["preds_neutralized"] = scaler.fit_transform(df2[["preds_neutralized"]]) # transform back to 0-1
Now our single feature exposures are much smaller
corr_list2 = []
for feature in features:
    corr_list2.append(np.corrcoef(df2[feature], df2["preds_neutralized"])[0,1])
corr_series2 = pd.Series(corr_list2, index=features)
corr_series2.describe()
count    1050.000000
mean        0.005275
std         0.062161
min        -0.314072
25%        -0.035640
50%         0.006457
75%         0.047849
max         0.144363
dtype: float64
Our overall score goes down, but the scores are more consistent than before. This leads to a higher sharpe
unbalanced_scores_per_era = df2.groupby("era").apply(lambda d: np.corrcoef(d["preds"], d[target])[0,1])
balanced_scores_per_era = df2.groupby("era").apply(lambda d: np.corrcoef(d["preds_neutralized"], d[target])[0,1])

print(f"score for high feature exposure: {unbalanced_scores_per_era.mean()}")
print(f"score for balanced feature expo: {balanced_scores_per_era.mean()}")

print(f"std for high feature exposure: {unbalanced_scores_per_era.std(ddof=0)}")
print(f"std for balanced feature expo: {balanced_scores_per_era.std(ddof=0)}")

print(f"sharpe for high feature exposure: {unbalanced_scores_per_era.mean()/unbalanced_scores_per_era.std(ddof=0)}")
print(f"sharpe for balanced feature expo: {balanced_scores_per_era.mean()/balanced_scores_per_era.std(ddof=0)}")
score for high feature exposure: 0.05233661570835508
score for balanced feature expo: 0.04681538564864233
std for high feature exposure: 0.022859528419203096
std for balanced feature expo: 0.019806139929550184
sharpe for high feature exposure: 2.2894879871795526
sharpe for balanced feature expo: 2.3636804453145936

balanced_scores_per_era.describe()
count    67.000000
mean      0.046815
std       0.019956
min       0.002495
25%       0.032911
50%       0.047356
75%       0.058174
max       0.105836
dtype: float64
unbalanced_scores_per_era.describe()
count    67.000000
mean      0.052337
std       0.023032
min       0.004386
25%       0.035233
50%       0.055631
75%       0.065317
max       0.116914
dtype: float64
We can also try to be more clever about what features we neutralize
The "riskiest" features are the features the predictions are most exposed to that also have the highest volatility
# compute feature correlations with target on the first half data
all_feature_corrs = df1.groupby('era').apply(lambda d: d[features].corrwith(d[target]))
# compute the volatility of the feature correlations
feature_corr_volatility = all_feature_corrs.std()
# calculate the feature exposures of the predictions
feature_exposure_list = []
for feature in features:
    feature_exposure_list.append(np.corrcoef(df2[feature], lgb_preds)[0,1])
feature_exposure_list = pd.Series(feature_exposure_list, index=features)
# get list of 100 riskiest features
riskiest_features = (feature_exposure_list.abs()*feature_corr_volatility).sort_values()[-100:].index.tolist()
# 50% neutralize to the riskiest features
df2["preds_neutralized_riskiest_100"] = neutralize(df2, 
                                                  columns=["preds"], 
                                                  neutralizers=riskiest_features, 
                                                  proportion=0.5, # 50% neutralize only those 100 features
                                                  normalize=True, 
                                                  era_col='era')

scaler = MinMaxScaler()
df2["preds_neutralized_riskiest_100"] = scaler.fit_transform(df2[["preds_neutralized_riskiest_100"]]) # transform back to 0-1
0573
<ipython-input-78-a2ec8b9e6f35>:2: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  df2["preds_neutralized_riskiest_100"] = neutralize(df2,
<ipython-input-78-a2ec8b9e6f35>:10: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  df2["preds_neutralized_riskiest_100"] = scaler.fit_transform(df2[["preds_neutralized_riskiest_100"]]) # transform back to 0-1
The score goes down less and the sharpe goes up more than 50% neutralizing all the features
unbalanced_scores_per_era = df2.groupby("era").apply(lambda d: np.corrcoef(d["preds"], d[target])[0,1])
balanced_scores_per_era = df2.groupby("era").apply(lambda d: np.corrcoef(d["preds_neutralized_riskiest_100"], d[target])[0,1])

print(f"score for high feature exposure: {unbalanced_scores_per_era.mean()}")
print(f"score for balanced feature expo: {balanced_scores_per_era.mean()}")

print(f"std for high feature exposure: {unbalanced_scores_per_era.std(ddof=0)}")
print(f"std for balanced feature expo: {balanced_scores_per_era.std(ddof=0)}")

print(f"sharpe for high feature exposure: {unbalanced_scores_per_era.mean()/unbalanced_scores_per_era.std(ddof=0)}")
print(f"sharpe for balanced feature expo: {balanced_scores_per_era.mean()/balanced_scores_per_era.std(ddof=0)}")
score for high feature exposure: 0.05233661570835508
score for balanced feature expo: 0.04784620200240574
std for high feature exposure: 0.022859528419203096
std for balanced feature expo: 0.01865889557683214
sharpe for high feature exposure: 2.2894879871795526
sharpe for balanced feature expo: 2.5642569146382965

Interestingly, the maximal feature exposure magnitude is actually lower (~0.28 vs ~0.31) than when 50% neutralizing all features
corr_list3 = []
for feature in features:
    corr_list3.append(np.corrcoef(df2[feature], df2["preds_neutralized_riskiest_100"])[0,1])
corr_series3 = pd.Series(corr_list3, index=features)
corr_series3.describe()
count    1050.000000
mean        0.010188
std         0.058860
min        -0.269052
25%        -0.029323
50%         0.012539
75%         0.050637
max         0.137796
dtype: float64
corr_series3.describe()
count    1050.000000
mean        0.010188
std         0.058860
min        -0.269052
25%        -0.029323
50%         0.012539
75%         0.050637
max         0.137796
dtype: float64
Training on the alternative targets can result in models with different patterns of feature exposures
This is a great way to go after MMC and very useful for ensembling
lgb = lightgbm.LGBMRegressor(n_jobs=-1, learning_rate=0.01, n_estimators=1000, max_depth=5)
lgb.fit(df1[features], df1['target'])
LGBMRegressor(learning_rate=0.01, max_depth=5, n_estimators=1000)
lgb_jerome = lightgbm.LGBMRegressor(n_jobs=-1, learning_rate=0.01, n_estimators=1000, max_depth=5)
lgb_jerome.fit(df1[features], df1['target_jerome_20'])
LGBMRegressor(learning_rate=0.01, max_depth=5, n_estimators=1000)
# The feature importances for each model are about 63% correlated
plt.figure(figsize = (16,6))
plt.subplot(1,2,1)
plt.plot(lgb.feature_importances_, alpha=.5)
plt.plot(lgb_jerome.feature_importances_, alpha=.5)
plt.subplot(1,2,2)
plt.scatter(lgb.feature_importances_, lgb_jerome.feature_importances_)

print(np.corrcoef([lgb.feature_importances_, lgb_jerome.feature_importances_])[0,1])
0.6270969845588339

lgb_preds = lgb.predict(df2[features])
lgb_jerome_preds = lgb_jerome.predict(df2[features])
# The predictions are about 75% correlated
np.corrcoef([lgb_preds, lgb_jerome_preds])[0,1]
0.7510865174862386
df2["preds"] = lgb_preds
df2["preds_jerome"] = lgb_jerome_preds
<ipython-input-88-abbdda99d361>:1: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  df2["preds"] = lgb_preds
<ipython-input-88-abbdda99d361>:2: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  df2["preds_jerome"] = lgb_jerome_preds
# 50% neutralize to riskest features for each model as above
for preds in ["preds", "preds_jerome"]:
    feature_exposure_list = []
    for feature in features:
        feature_exposure_list.append(np.corrcoef(df2[feature], df2[preds])[0,1])
    feature_exposure_list = pd.Series(feature_exposure_list, index=features)

    riskiest_features = (feature_exposure_list.abs()*feature_corr_volatility).sort_values()[-100:].index.tolist()
    
    df2[f"{preds}_neutralized_riskiest_100"] = neutralize(df2, 
                                                  columns=[f"{preds}"], 
                                                  neutralizers=riskiest_features, 
                                                  proportion=0.5, # 50% neutralize only those 100 features
                                                  normalize=True, 
                                                  era_col='era')
0573
<ipython-input-89-867f2bde3bb9>:10: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  df2[f"{preds}_neutralized_riskiest_100"] = neutralize(df2,
0573
# create ensemble
df2["preds_ens"] = (df2["preds"] + df2["preds_jerome"])/2
<ipython-input-90-c7a741c556d2>:2: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  df2["preds_ens"] = (df2["preds"] + df2["preds_jerome"])/2
scores = df2.groupby("era").apply(lambda d: np.corrcoef(d["preds"], d[target])[0,1])
scores_jerome = df2.groupby("era").apply(lambda d: np.corrcoef(d["preds_jerome"], d[target])[0,1])

scores_derisked = df2.groupby("era").apply(lambda d: np.corrcoef(d["preds_neutralized_riskiest_100"], d[target])[0,1])
scores_jerome_derisked = df2.groupby("era").apply(lambda d: np.corrcoef(d["preds_jerome_neutralized_riskiest_100"], d[target])[0,1])

scores_ens = df2.groupby("era").apply(lambda d: np.corrcoef(d["preds_ens"], d[target])[0,1])

print(f"score for target: {scores.mean()}")
print(f"score for target_jerome: {scores_jerome.mean()}")
print()
print(f"sharpe for target: {scores.mean()/scores.std(ddof=0)}")
print(f"sharpe for target_jerome: {scores_jerome.mean()/scores_jerome.std(ddof=0)}")
print()
print(f"score for target after neutralization: {scores_derisked.mean()}")
print(f"score for target_jerome after neutralization: {scores_jerome_derisked.mean()}")
print()
print(f"sharpe for target after neutralization: {scores_derisked.mean()/scores_derisked.std(ddof=0)}")
print(f"sharpe for target_jerome after neutralization: {scores_jerome_derisked.mean()/scores_jerome_derisked.std(ddof=0)}")
print()
print(f"score for target + target_jerome ensemble: {scores_ens.mean()}")
print(f"sharpe for target + target_jerome ensemble: {scores_ens.mean()/scores_ens.std(ddof=0)}")
score for target: 0.05233661570835508
score for target_jerome: 0.05293083310575564

sharpe for target: 2.2894879871795526
sharpe for target_jerome: 1.9501765858045648

score for target after neutralization: 0.04784620200240574
score for target_jerome after neutralization: 0.050838690600204337

sharpe for target after neutralization: 2.5642569146382965
sharpe for target_jerome after neutralization: 2.2734387549255732

score for target + target_jerome ensemble: 0.05627261521740282
sharpe for target + target_jerome ensemble: 2.1496760967783635

We see here that the model trained on target_jerome actually predicts target slightly better than the model trained on target (albeit with a lower Sharpe) both before and after neutralization of the riskiest features
# calculate correlation of neutralized predictions
df2[[f"preds_neutralized_riskiest_100", f"preds_jerome_neutralized_riskiest_100"]].corr()
Given that the neutralized version trained on target_jerome gets slightly better average score, but is only about 67% correlated with the neutralized version trained on target, it is an excellent candidate for MMC
However, ensembling the two neutralized predictions gives a model with the highest average score and a great Sharpe value
 
 
